{"./":{"url":"./","title":"Hudi 0.5.0 介绍","keywords":"","body":"什么是Hudi? Hudi为大数据带来流处理，在提供新数据的同时，比传统的批处理效率高出一个数量级。 Hudi（发音为“hoodie”）摄取与管理处于DFS(HDFS 或云存储)之上的大型分析数据集并为查询访问提供三个逻辑视图。 读优化视图 - 在纯列式存储上提供出色的查询性能，非常像parquet表。 增量视图 - 在数据集之上提供一个变更流并提供给下游的作业或ETL任务。 准实时的表 - 使用基于列存储和行存储(例如 Parquet + Avro)以提供对实时数据的查询 通过仔细地管理数据在存储中的布局和如何将数据暴露给查询，Hudi支持丰富的数据生态系统，在该系统中，外部数据源可被近实时摄取并被用于presto和spark等交互式SQL引擎，同时能够从处理/ETL框架（如hive和 spark中进行增量消费以构建派生（Hudi）数据集。 Hudi 大体上由一个自包含的Spark库组成，它用于构建数据集并与现有的数据访问查询引擎集成。有关演示，请参见快速开始。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"quickstart.html":{"url":"quickstart.html","title":"快速入门","keywords":"","body":" 本指南通过使用spark-shell简要介绍了Hudi功能。使用Spark数据源，我们将通过代码段展示如何插入和更新的Hudi默认存储类型数据集： 写时复制。每次写操作之后，我们还将展示如何读取快照和增量读取数据。 设置spark-shell Hudi适用于Spark-2.x版本。您可以按照此处的说明设置spark。 在提取的目录中，使用spark-shell运行Hudi： bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.0-incubating --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' 设置表名、基本路径和数据生成器来为本指南生成记录。 import org.apache.hudi.QuickstartUtils._ import scala.collection.JavaConversions._ import org.apache.spark.sql.SaveMode._ import org.apache.hudi.DataSourceReadOptions._ import org.apache.hudi.DataSourceWriteOptions._ import org.apache.hudi.config.HoodieWriteConfig._ val tableName = \"hudi_cow_table\" val basePath = \"file:///tmp/hudi_cow_table\" val dataGen = new DataGenerator 数据生成器 可以基于行程样本模式 生成插入和更新的样本。 插入数据 生成一些新的行程样本，将其加载到DataFrame中，然后将DataFrame写入Hudi数据集中，如下所示。 val inserts = convertToStringList(dataGen.generateInserts(10)) val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2)) df.write.format(\"org.apache.hudi\"). options(getQuickstartWriteConfigs). option(PRECOMBINE_FIELD_OPT_KEY, \"ts\"). option(RECORDKEY_FIELD_OPT_KEY, \"uuid\"). option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). option(TABLE_NAME, tableName). mode(Overwrite). save(basePath); mode(Overwrite)覆盖并重新创建数据集(如果已经存在)。 您可以检查在/tmp/hudi_cow_table////下生成的数据。我们提供了一个记录键 (schema中的uuid)，分区字段(region/county/city）和组合逻辑(schema中的ts) 以确保行程记录在每个分区中都是唯一的。更多信息请参阅 对Hudi中的数据进行建模-HowdoImodelthedatastoredinHudi?)， 有关将数据提取到Hudi中的方法的信息，请参阅写入Hudi数据集。 这里我们使用默认的写操作：插入更新。 如果您的工作负载没有更新，也可以使用更快的插入或批量插入操作。 想了解更多信息，请参阅写操作 查询数据 将数据文件加载到DataFrame中。 val roViewDF = spark. read. format(\"org.apache.hudi\"). load(basePath + \"/*/*/*/*\") roViewDF.registerTempTable(\"hudi_ro_table\") spark.sql(\"select fare, begin_lon, begin_lat, ts from hudi_ro_table where fare > 20.0\").show() spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from hudi_ro_table\").show() 该查询提供已提取数据的读取优化视图。由于我们的分区路径(region/country/city)是嵌套的3个级别 从基本路径开始，我们使用了load(basePath + \"/*/*/*/*\")。 有关支持的所有存储类型和视图的更多信息，请参考存储类型和视图。 更新数据 这类似于插入新数据。使用数据生成器生成对现有行程的更新，加载到DataFrame中并将DataFrame写入hudi数据集。 val updates = convertToStringList(dataGen.generateUpdates(10)) val df = spark.read.json(spark.sparkContext.parallelize(updates, 2)); df.write.format(\"org.apache.hudi\"). options(getQuickstartWriteConfigs). option(PRECOMBINE_FIELD_OPT_KEY, \"ts\"). option(RECORDKEY_FIELD_OPT_KEY, \"uuid\"). option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). option(TABLE_NAME, tableName). mode(Append). save(basePath); 注意，保存模式现在为追加。通常，除非您是第一次尝试创建数据集，否则请始终使用追加模式。 查询现在再次查询数据将显示更新的行程。每个写操作都会生成一个新的由时间戳表示的commit 。在之前提交的相同的_hoodie_record_key中寻找_hoodie_commit_time, rider, driver字段变更。 增量查询 Hudi还提供了获取给定提交时间戳以来已更改的记录流的功能。 这可以通过使用Hudi的增量视图并提供所需更改的开始时间来实现。 如果我们需要给定提交之后的所有更改(这是常见的情况)，则无需指定结束时间。 // reload data spark. read. format(\"org.apache.hudi\"). load(basePath + \"/*/*/*/*\"). createOrReplaceTempView(\"hudi_ro_table\") val commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_ro_table order by commitTime\").map(k => k.getString(0)).take(50) val beginTime = commits(commits.length - 2) // commit time we are interested in // 增量查询数据 val incViewDF = spark. read. format(\"org.apache.hudi\"). option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL). option(BEGIN_INSTANTTIME_OPT_KEY, beginTime). load(basePath); incViewDF.registerTempTable(\"hudi_incr_table\") spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_incr_table where fare > 20.0\").show() 这将提供在开始时间提交之后发生的所有更改，其中包含票价大于20.0的过滤器。关于此功能的独特之处在于，它现在使您可以在批量数据上创作流式管道。 特定时间点查询 让我们看一下如何查询特定时间的数据。可以通过将结束时间指向特定的提交时间，将开始时间指向\"000\"(表示最早的提交时间)来表示特定时间。 val beginTime = \"000\" // Represents all commits > this time. val endTime = commits(commits.length - 2) // commit time we are interested in // 增量查询数据 val incViewDF = spark.read.format(\"org.apache.hudi\"). option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL). option(BEGIN_INSTANTTIME_OPT_KEY, beginTime). option(END_INSTANTTIME_OPT_KEY, endTime). load(basePath); incViewDF.registerTempTable(\"hudi_incr_table\") spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_incr_table where fare > 20.0\").show() 从这开始下一步？ 您也可以通过自己构建hudi来快速开始， 并在spark-shell命令中使用--jars /packaging/hudi-spark-bundle/target/hudi-spark-bundle-*.*.*-SNAPSHOT.jar， 而不是--packages org.apache.hudi:hudi-spark-bundle:0.5.0-incubating 这里我们使用Spark演示了Hudi的功能。但是，Hudi可以支持多种存储类型/视图，并且可以从Hive，Spark，Presto等查询引擎中查询Hudi数据集。 我们制作了一个基于Docker设置、所有依赖系统都在本地运行的演示视频， 我们建议您复制相同的设置然后按照这里的步骤自己运行这个演示。 另外，如果您正在寻找将现有数据迁移到Hudi的方法，请参考迁移指南。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"use_cases.html":{"url":"use_cases.html","title":"使用案例","keywords":"","body":"近实时摄取 将外部源(如事件日志、数据库、外部源)的数据摄取到Hadoop数据湖是一个众所周知的问题。 尽管这些数据对整个组织来说是最有价值的，但不幸的是，在大多数(如果不是全部)Hadoop部署中都使用零散的方式解决，即使用多个不同的摄取工具。 对于RDBMS摄取，Hudi提供 通过更新插入达到更快加载，而不是昂贵且低效的批量加载。例如，您可以读取MySQL BIN日志或Sqoop增量导入并将其应用于 DFS上的等效Hudi表。这比批量合并任务及复杂的手工合并工作流更快/更有效率。 对于NoSQL数据存储，如Cassandra / Voldemort / HBase，即使是中等规模大小也会存储数十亿行。 毫无疑问， 全量加载不可行，如果摄取需要跟上较高的更新量，那么则需要更有效的方法。 即使对于像Kafka这样的不可变数据源，Hudi也可以 强制在HDFS上使用最小文件大小, 这采取了综合方式解决HDFS小文件问题来改善NameNode的健康状况。这对事件流来说更为重要，因为它通常具有较高容量(例如：点击流)，如果管理不当，可能会对Hadoop集群造成严重损害。 在所有源中，通过commits这一概念，Hudi增加了以原子方式向消费者发布新数据的功能，这种功能十分必要。 近实时分析 通常，实时数据集市由专业(实时)数据分析存储提供支持，例如Druid或Memsql或OpenTSDB。 这对于较小规模的数据量来说绝对是完美的(相比于这样安装Hadoop)，这种情况需要在亚秒级响应查询，例如系统监控或交互式实时分析。 但是，由于Hadoop上的数据太陈旧了，通常这些系统会被滥用于非交互式查询，这导致利用率不足和硬件/许可证成本的浪费。 另一方面，Hadoop上的交互式SQL解决方案(如Presto和SparkSQL)表现出色，在 几秒钟内完成查询。 通过将 数据新鲜度提高到几分钟，Hudi可以提供一个更有效的替代方案，并支持存储在DFS中的 数量级更大的数据集 的实时分析。 此外，Hudi没有外部依赖(如专用于实时分析的HBase集群)，因此可以在更新的分析上实现更快的分析，而不会增加操作开销。 增量处理管道 Hadoop提供的一个基本能力是构建一系列数据集，这些数据集通过表示为工作流的DAG相互派生。 工作流通常取决于多个上游工作流输出的新数据，新数据的可用性传统上由新的DFS文件夹/Hive分区指示。 让我们举一个具体的例子来说明这点。上游工作流U可以每小时创建一个Hive分区，在每小时结束时(processing_time)使用该小时的数据(event_time)，提供1小时的有效新鲜度。 然后，下游工作流D在U结束后立即启动，并在下一个小时内自行处理，将有效延迟时间增加到2小时。 上面的示例忽略了迟到的数据，即processing_time和event_time分开时。 不幸的是，在今天的后移动和前物联网世界中，来自间歇性连接的移动设备和传感器的延迟数据是常态，而不是异常。 在这种情况下，保证正确性的唯一补救措施是重新处理最后几个小时的数据， 每小时一遍又一遍，这可能会严重影响整个生态系统的效率。例如; 试想一下，在数百个工作流中每小时重新处理TB数据。 Hudi通过以单个记录为粒度的方式(而不是文件夹/分区)从上游 Hudi数据集HU消费新数据(包括迟到数据)，来解决上面的问题。 应用处理逻辑，并使用下游Hudi数据集HD高效更新/协调迟到数据。在这里，HU和HD可以以更频繁的时间被连续调度 比如15分钟，并且HD提供端到端30分钟的延迟。 为实现这一目标，Hudi采用了类似于Spark Streaming、发布/订阅系统等流处理框架，以及像Kafka 或Oracle XStream等数据库复制技术的类似概念。 如果感兴趣，可以在这里找到有关增量处理(相比于流处理和批处理)好处的更详细解释。 DFS的数据分发 一个常用场景是先在Hadoop上处理数据，然后将其分发回在线服务存储层，以供应用程序使用。 例如，一个Spark管道可以确定Hadoop上的紧急制动事件并将它们加载到服务存储层(如ElasticSearch)中，供Uber应用程序使用以增加安全驾驶。这种用例中，通常架构会在Hadoop和服务存储之间引入队列，以防止目标服务存储被压垮。 对于队列的选择，一种流行的选择是Kafka，这个模型经常导致 在DFS上存储相同数据的冗余(用于计算结果的离线分析)和Kafka(用于分发) 通过将每次运行的Spark管道更新插入的输出转换为Hudi数据集，Hudi可以再次有效地解决这个问题，然后可以以增量方式获取尾部数据(就像Kafka topic一样)然后写入服务存储层。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"powered_by.html":{"url":"powered_by.html","title":"演讲 & Hudi 用户","keywords":"","body":"已使用 Uber Hudi最初由Uber开发，用于实现低延迟、高效率的数据库摄取。 Hudi自2016年8月开始在生产环境上线，在Hadoop上驱动约100个非常关键的业务表，支撑约几百TB的数据规模(前10名包括行程、乘客、司机)。 Hudi还支持几个增量的Hive ETL管道，并且目前已集成到Uber的数据分发系统中。 EMIS Health EMIS Health是英国最大的初级保健IT软件提供商，其数据集包括超过5000亿的医疗保健记录。HUDI用于管理生产中的分析数据集，并使其与上游源保持同步。Presto用于查询以HUDI格式写入的数据。 Yields.io Yields.io是第一个使用AI在企业范围内进行自动模型验证和实时监控的金融科技平台。他们的数据湖由Hudi管理，他们还积极使用Hudi为增量式、跨语言/平台机器学习构建基础架构。 Yotpo Hudi在Yotpo有不少用途。首先，在他们的开源ETL框架中集成了Hudi作为CDC管道的输出写入程序，即从数据库binlog生成的事件流到Kafka然后再写入S3。 演讲 & 报告 \"Hoodie: Incremental processing on Hadoop at Uber\" - By Vinoth Chandar & Prasanna Rajaperumal Mar 2017, Strata + Hadoop World, San Jose, CA \"Hoodie: An Open Source Incremental Processing Framework From Uber\" - By Vinoth Chandar. Apr 2017, DataEngConf, San Francisco, CA Slides Video \"Incremental Processing on Large Analytical Datasets\" - By Prasanna Rajaperumal June 2017, Spark Summit 2017, San Francisco, CA. Slides Video \"Hudi: Unifying storage and serving for batch and near-real-time analytics\" - By Nishith Agarwal & Balaji Vardarajan September 2018, Strata Data Conference, New York, NY \"Hudi: Large-Scale, Near Real-Time Pipelines at Uber\" - By Vinoth Chandar & Nishith Agarwal October 2018, Spark+AI Summit Europe, London, UK \"Powering Uber's global network analytics pipelines in real-time with Apache Hudi\" - By Ethan Guo & Nishith Agarwal, April 2019, Data Council SF19, San Francisco, CA. \"Building highly efficient data lakes using Apache Hudi (Incubating)\" - By Vinoth Chandar June 2019, SF Big Analytics Meetup, San Mateo, CA \"Apache Hudi (Incubating) - The Past, Present and Future Of Efficient Data Lake Architectures\" - By Vinoth Chandar & Balaji Varadarajan September 2019, ApacheCon NA 19, Las Vegas, NV, USA 文章 \"The Case for incremental processing on Hadoop\" - O'reilly Ideas article by Vinoth Chandar \"Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop\" - Engineering Blog By Prasanna Rajaperumal 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"comparison.html":{"url":"comparison.html","title":"对比","keywords":"","body":"Apache Hudi填补了在DFS上处理数据的巨大空白，并可以和这些技术很好地共存。然而， 通过将Hudi与一些相关系统进行对比，来了解Hudi如何适应当前的大数据生态系统，并知晓这些系统在设计中做的不同权衡仍将非常有用。 Kudu Apache Kudu是一个与Hudi具有相似目标的存储系统，该系统通过对upserts支持来对PB级数据进行实时分析。 一个关键的区别是Kudu还试图充当OLTP工作负载的数据存储，而Hudi并不希望这样做。 因此，Kudu不支持增量拉取(截至2017年初)，而Hudi支持以便进行增量处理。 Kudu与分布式文件系统抽象和HDFS完全不同，它自己的一组存储服务器通过RAFT相互通信。 与之不同的是，Hudi旨在与底层Hadoop兼容的文件系统(HDFS，S3或Ceph)一起使用，并且没有自己的存储服务器群，而是依靠Apache Spark来完成繁重的工作。 因此，Hudi可以像其他Spark作业一样轻松扩展，而Kudu则需要硬件和运营支持，特别是HBase或Vertica等数据存储系统。 到目前为止，我们还没有做任何直接的基准测试来比较Kudu和Hudi(鉴于RTTable正在进行中)。 但是，如果我们要使用CERN， 我们预期Hudi在摄取parquet上有更卓越的性能。 Hive事务 Hive事务/ACID是另一项类似的工作，它试图实现在ORC文件格式之上的存储读取时合并。 可以理解，此功能与Hive以及LLAP之类的其他工作紧密相关。 Hive事务不提供Hudi提供的读取优化存储选项或增量拉取。 在实现选择方面，Hudi充分利用了类似Spark的处理框架的功能，而Hive事务特性则在用户或Hive Metastore启动的Hive任务/查询的下实现。 根据我们的生产经验，与其他方法相比，将Hudi作为库嵌入到现有的Spark管道中要容易得多，并且操作不会太繁琐。 Hudi还设计用于与Presto/Spark等非Hive引擎合作，并计划引入除parquet以外的文件格式。 HBase 尽管HBase最终是OLTP工作负载的键值存储层，但由于与Hadoop的相似性，用户通常倾向于将HBase与分析相关联。 鉴于HBase经过严格的写优化，它支持开箱即用的亚秒级更新，Hive-on-HBase允许用户查询该数据。 但是，就分析工作负载的实际性能而言，Parquet/ORC之类的混合列式存储格式可以轻松击败HBase，因为这些工作负载主要是读取繁重的工作。 Hudi弥补了更快的数据与分析存储格式之间的差距。从运营的角度来看，与管理分析使用的HBase region服务器集群相比，为用户提供可更快给出数据的库更具可扩展性。 最终，HBase不像Hudi这样重点支持提交时间、增量拉取之类的增量处理原语。 流式处理 一个普遍的问题：\"Hudi与流处理系统有何关系？\"，我们将在这里尝试回答。简而言之，Hudi可以与当今的批处理(写时复制存储)和流处理(读时合并存储)作业集成，以将计算结果存储在Hadoop中。 对于Spark应用程序，这可以通过将Hudi库与Spark/Spark流式DAG直接集成来实现。在非Spark处理系统(例如Flink、Hive)情况下，可以在相应的系统中进行处理，然后通过Kafka主题/DFS中间文件将其发送到Hudi表中。从概念上讲，数据处理 管道仅由三个部分组成：输入，处理，输出，用户最终针对输出运行查询以便使用管道的结果。Hudi可以充当将数据存储在DFS上的输入或输出。Hudi在给定流处理管道上的适用性最终归结为你的查询在Presto/SparkSQL/Hive的适用性。 更高级的用例围绕增量处理的概念展开， 甚至在处理引擎内部也使用Hudi来加速典型的批处理管道。例如：Hudi可用作DAG内的状态存储(类似Flink使用的[rocksDB(https://ci.apache.org/projects/flink/flink-docs-release-1.2/ops/state_backends.html#the-rocksdbstatebackend))。 这是路线图上的一个项目并将最终以Beam Runner的形式呈现。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"docker_demo.html":{"url":"docker_demo.html","title":"Docker 示例","keywords":"","body":"A Demo using docker containers Lets use a real world example to see how hudi works end to end. For this purpose, a self contained data infrastructure is brought up in a local docker cluster within your computer. The steps have been tested on a Mac laptop Prerequisites Docker Setup : For Mac, Please follow the steps as defined in [https://docs.docker.com/v17.12/docker-for-mac/install/]. For running Spark-SQL queries, please ensure atleast 6 GB and 4 CPUs are allocated to Docker (See Docker -> Preferences -> Advanced). Otherwise, spark-SQL queries could be killed because of memory issues. kafkacat : A command-line utility to publish/consume from kafka topics. Use brew install kafkacat to install kafkacat /etc/hosts : The demo references many services running in container by the hostname. Add the following settings to /etc/hosts 127.0.0.1 adhoc-1 127.0.0.1 adhoc-2 127.0.0.1 namenode 127.0.0.1 datanode1 127.0.0.1 hiveserver 127.0.0.1 hivemetastore 127.0.0.1 kafkabroker 127.0.0.1 sparkmaster 127.0.0.1 zookeeper Also, this has not been tested on some environments like Docker on Windows. Setting up Docker Cluster Build Hudi The first step is to build hudi cd mvn package -DskipTests Bringing up Demo Cluster The next step is to run the docker compose script and setup configs for bringing up the cluster. This should pull the docker images from docker hub and setup docker cluster. cd docker ./setup_demo.sh .... .... .... Stopping spark-worker-1 ... done Stopping hiveserver ... done Stopping hivemetastore ... done Stopping historyserver ... done ....... ...... Creating network \"hudi_demo\" with the default driver Creating hive-metastore-postgresql ... done Creating namenode ... done Creating zookeeper ... done Creating kafkabroker ... done Creating hivemetastore ... done Creating historyserver ... done Creating hiveserver ... done Creating datanode1 ... done Creating sparkmaster ... done Creating adhoc-1 ... done Creating adhoc-2 ... done Creating spark-worker-1 ... done Copying spark default config and setting up configs Copying spark default config and setting up configs Copying spark default config and setting up configs varadarb-C02SG7Q3G8WP:docker varadarb$ docker ps At this point, the docker cluster will be up and running. The demo cluster brings up the following services HDFS Services (NameNode, DataNode) Spark Master and Worker Hive Services (Metastore, HiveServer2 along with PostgresDB) Kafka Broker and a Zookeeper Node (Kakfa will be used as upstream source for the demo) Adhoc containers to run Hudi/Hive CLI commands Demo Stock Tracker data will be used to showcase both different Hudi Views and the effects of Compaction. Take a look at the directory docker/demo/data. There are 2 batches of stock data - each at 1 minute granularity. The first batch contains stocker tracker data for some stock symbols during the first hour of trading window (9:30 a.m to 10:30 a.m). The second batch contains tracker data for next 30 mins (10:30 - 11 a.m). Hudi will be used to ingest these batches to a dataset which will contain the latest stock tracker data at hour level granularity. The batches are windowed intentionally so that the second batch contains updates to some of the rows in the first batch. Step 1 : Publish the first batch to Kafka Upload the first batch to Kafka topic 'stock ticks' cat docker/demo/data/batch_1.json | kafkacat -b kafkabroker -t stock_ticks -P To check if the new topic shows up, use kafkacat -b kafkabroker -L -J | jq . { \"originating_broker\": { \"id\": 1001, \"name\": \"kafkabroker:9092/1001\" }, \"query\": { \"topic\": \"*\" }, \"brokers\": [ { \"id\": 1001, \"name\": \"kafkabroker:9092\" } ], \"topics\": [ { \"topic\": \"stock_ticks\", \"partitions\": [ { \"partition\": 0, \"leader\": 1001, \"replicas\": [ { \"id\": 1001 } ], \"isrs\": [ { \"id\": 1001 } ] } ] } ] } Step 2: Incrementally ingest data from Kafka topic Hudi comes with a tool named DeltaStreamer. This tool can connect to variety of data sources (including Kafka) to pull changes and apply to Hudi dataset using upsert/insert primitives. Here, we will use the tool to download json data from kafka topic and ingest to both COW and MOR tables we initialized in the previous step. This tool automatically initializes the datasets in the file-system if they do not exist yet. docker exec -it adhoc-2 /bin/bash # Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_cow dataset in HDFS spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type COPY_ON_WRITE --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_cow --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider .... .... 2018-09-24 22:20:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped! 2018-09-24 22:20:00 INFO SparkContext:54 - Successfully stopped SparkContext # Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_mor dataset in HDFS spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction .... 2018-09-24 22:22:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped! 2018-09-24 22:22:01 INFO SparkContext:54 - Successfully stopped SparkContext .... # As part of the setup (Look at setup_demo.sh), the configs needed for DeltaStreamer is uploaded to HDFS. The configs # contain mostly Kafa connectivity settings, the avro-schema to be used for ingesting along with key and partitioning fields. exit You can use HDFS web-browser to look at the datasets http://namenode:50070/explorer.html#/user/hive/warehouse/stock_ticks_cow. You can explore the new partition folder created in the dataset along with a \"deltacommit\" file under .hoodie which signals a successful commit. There will be a similar setup when you browse the MOR dataset http://namenode:50070/explorer.html#/user/hive/warehouse/stock_ticks_mor Step 3: Sync with Hive At this step, the datasets are available in HDFS. We need to sync with Hive to create new Hive tables and add partitions inorder to run Hive queries against those datasets. docker exec -it adhoc-2 /bin/bash # THis command takes in HIveServer URL and COW Hudi Dataset location in HDFS and sync the HDFS state to Hive /var/hoodie/ws/hudi-hive/run_sync_tool.sh --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_cow --database default --table stock_ticks_cow ..... 2018-09-24 22:22:45,568 INFO [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_cow ..... # Now run hive-sync for the second data-set in HDFS using Merge-On-Read (MOR storage) /var/hoodie/ws/hudi-hive/run_sync_tool.sh --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor ... 2018-09-24 22:23:09,171 INFO [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_mor ... 2018-09-24 22:23:09,559 INFO [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_mor_rt .... exit After executing the above command, you will notice A hive table named stock_ticks_cow created which provides Read-Optimized view for the Copy On Write dataset. Two new tables stock_ticks_mor and stock_ticks_mor_rt created for the Merge On Read dataset. The former provides the ReadOptimized view for the Hudi dataset and the later provides the realtime-view for the dataset. Step 4 (a): Run Hive Queries Run a hive query to find the latest timestamp ingested for stock symbol 'GOOG'. You will notice that both read-optimized (for both COW and MOR dataset)and realtime views (for MOR dataset)give the same value \"10:29 a.m\" as Hudi create a parquet file for the first batch of data. docker exec -it adhoc-2 /bin/bash beeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false # List Tables 0: jdbc:hive2://hiveserver:10000> show tables; +---------------------+--+ | tab_name | +---------------------+--+ | stock_ticks_cow | | stock_ticks_mor | | stock_ticks_mor_rt | +---------------------+--+ 2 rows selected (0.801 seconds) 0: jdbc:hive2://hiveserver:10000> # Look at partitions that were added 0: jdbc:hive2://hiveserver:10000> show partitions stock_ticks_mor_rt; +----------------+--+ | partition | +----------------+--+ | dt=2018-08-31 | +----------------+--+ 1 row selected (0.24 seconds) # COPY-ON-WRITE Queries: ========================= 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'; +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:29:00 | +---------+----------------------+--+ Now, run a projection query: 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924221953 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924221953 | GOOG | 2018-08-31 10:29:00 | 3391 | 1230.1899 | 1230.085 | +----------------------+---------+----------------------+---------+------------+-----------+--+ # Merge-On-Read Queries: ========================== Lets run similar queries against M-O-R dataset. Lets look at both ReadOptimized and Realtime views supported by M-O-R dataset # Run against ReadOptimized View. Notice that the latest timestamp is 10:29 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:29:00 | +---------+----------------------+--+ 1 row selected (6.326 seconds) # Run against Realtime View. Notice that the latest timestamp is again 10:29 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:29:00 | +---------+----------------------+--+ 1 row selected (1.606 seconds) # Run projection query against Read Optimized and Realtime tables 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924222155 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924222155 | GOOG | 2018-08-31 10:29:00 | 3391 | 1230.1899 | 1230.085 | +----------------------+---------+----------------------+---------+------------+-----------+--+ 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor_rt where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924222155 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924222155 | GOOG | 2018-08-31 10:29:00 | 3391 | 1230.1899 | 1230.085 | +----------------------+---------+----------------------+---------+------------+-----------+--+ exit exit Step 4 (b): Run Spark-SQL Queries Hudi support Spark as query processor just like Hive. Here are the same hive queries running in spark-sql docker exec -it adhoc-1 /bin/bash $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --master local[2] --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client --driver-memory 1G --executor-memory 3G --num-executors 1 --packages com.databricks:spark-avro_2.11:4.0.0 ... Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.3.1 /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181) Type in expressions to have them evaluated. Type :help for more information. scala> scala> spark.sql(\"show tables\").show(100, false) +--------+------------------+-----------+ |database|tableName |isTemporary| +--------+------------------+-----------+ |default |stock_ticks_cow |false | |default |stock_ticks_mor |false | |default |stock_ticks_mor_rt|false | +--------+------------------+-----------+ # Copy-On-Write Table ## Run max timestamp query against COW table scala> spark.sql(\"select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'\").show(100, false) [Stage 0:> (0 + 1) / 1]SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. +------+-------------------+ |symbol|max(ts) | +------+-------------------+ |GOOG |2018-08-31 10:29:00| +------+-------------------+ ## Projection Query scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG'\").show(100, false) +-------------------+------+-------------------+------+---------+--------+ |_hoodie_commit_time|symbol|ts |volume|open |close | +-------------------+------+-------------------+------+---------+--------+ |20180924221953 |GOOG |2018-08-31 09:59:00|6330 |1230.5 |1230.02 | |20180924221953 |GOOG |2018-08-31 10:29:00|3391 |1230.1899|1230.085| +-------------------+------+-------------------+------+---------+--------+ # Merge-On-Read Queries: ========================== Lets run similar queries against M-O-R dataset. Lets look at both ReadOptimized and Realtime views supported by M-O-R dataset # Run against ReadOptimized View. Notice that the latest timestamp is 10:29 scala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false) +------+-------------------+ |symbol|max(ts) | +------+-------------------+ |GOOG |2018-08-31 10:29:00| +------+-------------------+ # Run against Realtime View. Notice that the latest timestamp is again 10:29 scala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false) +------+-------------------+ |symbol|max(ts) | +------+-------------------+ |GOOG |2018-08-31 10:29:00| +------+-------------------+ # Run projection query against Read Optimized and Realtime tables scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG'\").show(100, false) +-------------------+------+-------------------+------+---------+--------+ |_hoodie_commit_time|symbol|ts |volume|open |close | +-------------------+------+-------------------+------+---------+--------+ |20180924222155 |GOOG |2018-08-31 09:59:00|6330 |1230.5 |1230.02 | |20180924222155 |GOOG |2018-08-31 10:29:00|3391 |1230.1899|1230.085| +-------------------+------+-------------------+------+---------+--------+ scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor_rt where symbol = 'GOOG'\").show(100, false) +-------------------+------+-------------------+------+---------+--------+ |_hoodie_commit_time|symbol|ts |volume|open |close | +-------------------+------+-------------------+------+---------+--------+ |20180924222155 |GOOG |2018-08-31 09:59:00|6330 |1230.5 |1230.02 | |20180924222155 |GOOG |2018-08-31 10:29:00|3391 |1230.1899|1230.085| +-------------------+------+-------------------+------+---------+--------+ Step 5: Upload second batch to Kafka and run DeltaStreamer to ingest Upload the second batch of data and ingest this batch using delta-streamer. As this batch does not bring in any new partitions, there is no need to run hive-sync cat docker/demo/data/batch_2.json | kafkacat -b kafkabroker -t stock_ticks -P # Within Docker container, run the ingestion command docker exec -it adhoc-2 /bin/bash # Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_cow dataset in HDFS spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type COPY_ON_WRITE --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_cow --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider # Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_mor dataset in HDFS spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction exit With Copy-On-Write table, the second ingestion by DeltaStreamer resulted in a new version of Parquet file getting created. See http://namenode:50070/explorer.html#/user/hive/warehouse/stock_ticks_cow/2018/08/31 With Merge-On-Read table, the second ingestion merely appended the batch to an unmerged delta (log) file. Take a look at the HDFS filesystem to get an idea: http://namenode:50070/explorer.html#/user/hive/warehouse/stock_ticks_mor/2018/08/31 Step 6(a): Run Hive Queries With Copy-On-Write table, the read-optimized view immediately sees the changes as part of second batch once the batch got committed as each ingestion creates newer versions of parquet files. With Merge-On-Read table, the second ingestion merely appended the batch to an unmerged delta (log) file. This is the time, when ReadOptimized and Realtime views will provide different results. ReadOptimized view will still return \"10:29 am\" as it will only read from the Parquet file. Realtime View will do on-the-fly merge and return latest committed data which is \"10:59 a.m\". docker exec -it adhoc-2 /bin/bash beeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false # Copy On Write Table: 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ 1 row selected (1.932 seconds) 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924221953 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924224524 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ As you can notice, the above queries now reflect the changes that came as part of ingesting second batch. # Merge On Read Table: # Read Optimized View 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:29:00 | +---------+----------------------+--+ 1 row selected (1.6 seconds) 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924222155 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924222155 | GOOG | 2018-08-31 10:29:00 | 3391 | 1230.1899 | 1230.085 | +----------------------+---------+----------------------+---------+------------+-----------+--+ # Realtime View 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor_rt where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924222155 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924224537 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ exit exit Step 6(b): Run Spark SQL Queries Running the same queries in Spark-SQL: docker exec -it adhoc-1 /bin/bash bash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1 --packages com.databricks:spark-avro_2.11:4.0.0 # Copy On Write Table: scala> spark.sql(\"select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'\").show(100, false) +------+-------------------+ |symbol|max(ts) | +------+-------------------+ |GOOG |2018-08-31 10:59:00| +------+-------------------+ scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG'\").show(100, false) +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924221953 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924224524 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ As you can notice, the above queries now reflect the changes that came as part of ingesting second batch. # Merge On Read Table: # Read Optimized View scala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false) +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:29:00 | +---------+----------------------+--+ 1 row selected (1.6 seconds) scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG'\").show(100, false) +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924222155 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924222155 | GOOG | 2018-08-31 10:29:00 | 3391 | 1230.1899 | 1230.085 | +----------------------+---------+----------------------+---------+------------+-----------+--+ # Realtime View scala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false) +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor_rt where symbol = 'GOOG'\").show(100, false) +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924222155 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924224537 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ exit exit Step 7 : Incremental Query for COPY-ON-WRITE Table With 2 batches of data ingested, lets showcase the support for incremental queries in Hudi Copy-On-Write datasets Lets take the same projection query example docker exec -it adhoc-2 /bin/bash beeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924064621 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924065039 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ As you notice from the above queries, there are 2 commits - 20180924064621 and 20180924065039 in timeline order. When you follow the steps, you will be getting different timestamps for commits. Substitute them in place of the above timestamps. To show the effects of incremental-query, let us assume that a reader has already seen the changes as part of ingesting first batch. Now, for the reader to see effect of the second batch, he/she has to keep the start timestamp to the commit time of the first batch (20180924064621) and run incremental query Hudi incremental mode provides efficient scanning for incremental queries by filtering out files that do not have any candidate rows using hudi-managed metadata. docker exec -it adhoc-2 /bin/bash beeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false 0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.mode=INCREMENTAL; No rows affected (0.009 seconds) 0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.max.commits=3; No rows affected (0.009 seconds) 0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.start.timestamp=20180924064621; With the above setting, file-ids that do not have any updates from the commit 20180924065039 is filtered out without scanning. Here is the incremental query : 0: jdbc:hive2://hiveserver:10000> 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG' and `_hoodie_commit_time` > '20180924064621'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924065039 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ 1 row selected (0.83 seconds) 0: jdbc:hive2://hiveserver:10000> Incremental Query with Spark SQL: docker exec -it adhoc-1 /bin/bash bash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1 --packages com.databricks:spark-avro_2.11:4.0.0 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.3.1 /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181) Type in expressions to have them evaluated. Type :help for more information. scala> import org.apache.hudi.DataSourceReadOptions import org.apache.hudi.DataSourceReadOptions # In the below query, 20180925045257 is the first commit's timestamp scala> val hoodieIncViewDF = spark.read.format(\"org.apache.hudi\").option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL).option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, \"20180924064621\").load(\"/user/hive/warehouse/stock_ticks_cow\") SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. hoodieIncViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 15 more fields] scala> hoodieIncViewDF.registerTempTable(\"stock_ticks_cow_incr_tmp1\") warning: there was one deprecation warning; re-run with -deprecation for details scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_cow_incr_tmp1 where symbol = 'GOOG'\").show(100, false); +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924065039 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ Step 8: Schedule and Run Compaction for Merge-On-Read dataset Lets schedule and run a compaction to create a new version of columnar file so that read-optimized readers will see fresher data. Again, You can use Hudi CLI to manually schedule and run compaction docker exec -it adhoc-1 /bin/bash root@adhoc-1:/opt# /var/hoodie/ws/hudi-cli/hudi-cli.sh ============================================ * * * _ _ _ _ * * | | | | | | (_) * * | |__| | __| | - * * | __ || | / _` | || * * | | | || || (_| | || * * |_| |_|\\___/ \\____/ || * * * ============================================ Welcome to Hoodie CLI. Please type help if you are looking for help. hudi->connect --path /user/hive/warehouse/stock_ticks_mor 18/09/24 06:59:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/09/24 06:59:35 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor 18/09/24 06:59:35 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]] 18/09/24 06:59:35 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties 18/09/24 06:59:36 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor Metadata for table stock_ticks_mor loaded # Ensure no compactions are present hoodie:stock_ticks_mor->compactions show all 18/09/24 06:59:54 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924064636__clean__COMPLETED], [20180924064636__deltacommit__COMPLETED], [20180924065057__clean__COMPLETED], [20180924065057__deltacommit__COMPLETED]] ___________________________________________________________________ | Compaction Instant Time| State | Total FileIds to be Compacted| |==================================================================| # Schedule a compaction. This will use Spark Launcher to schedule compaction hoodie:stock_ticks_mor->compaction schedule .... Compaction successfully completed for 20180924070031 # Now refresh and check again. You will see that there is a new compaction requested hoodie:stock_ticks->connect --path /user/hive/warehouse/stock_ticks_mor 18/09/24 07:01:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor 18/09/24 07:01:16 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]] 18/09/24 07:01:16 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties 18/09/24 07:01:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor Metadata for table stock_ticks_mor loaded hoodie:stock_ticks_mor->compactions show all 18/09/24 06:34:12 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924041125__clean__COMPLETED], [20180924041125__deltacommit__COMPLETED], [20180924042735__clean__COMPLETED], [20180924042735__deltacommit__COMPLETED], [==>20180924063245__compaction__REQUESTED]] ___________________________________________________________________ | Compaction Instant Time| State | Total FileIds to be Compacted| |==================================================================| | 20180924070031 | REQUESTED| 1 | # Execute the compaction. The compaction instant value passed below must be the one displayed in the above \"compactions show all\" query hoodie:stock_ticks_mor->compaction run --compactionInstant 20180924070031 --parallelism 2 --sparkMemory 1G --schemaFilePath /var/demo/config/schema.avsc --retry 1 .... Compaction successfully completed for 20180924070031 ## Now check if compaction is completed hoodie:stock_ticks_mor->connect --path /user/hive/warehouse/stock_ticks_mor 18/09/24 07:03:00 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor 18/09/24 07:03:00 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]] 18/09/24 07:03:00 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties 18/09/24 07:03:00 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor Metadata for table stock_ticks_mor loaded hoodie:stock_ticks->compactions show all 18/09/24 07:03:15 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924064636__clean__COMPLETED], [20180924064636__deltacommit__COMPLETED], [20180924065057__clean__COMPLETED], [20180924065057__deltacommit__COMPLETED], [20180924070031__commit__COMPLETED]] ___________________________________________________________________ | Compaction Instant Time| State | Total FileIds to be Compacted| |==================================================================| | 20180924070031 | COMPLETED| 1 | Step 9: Run Hive Queries including incremental queries You will see that both ReadOptimized and Realtime Views will show the latest committed data. Lets also run the incremental query for MOR table. From looking at the below query output, it will be clear that the fist commit time for the MOR table is 20180924064636 and the second commit time is 20180924070031 docker exec -it adhoc-2 /bin/bash beeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false # Read Optimized View 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ 1 row selected (1.6 seconds) 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924064636 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924070031 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ # Realtime View 0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor_rt where symbol = 'GOOG'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924064636 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924070031 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ # Incremental View: 0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.mode=INCREMENTAL; No rows affected (0.008 seconds) # Max-Commits covers both second batch and compaction commit 0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.max.commits=3; No rows affected (0.007 seconds) 0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.start.timestamp=20180924064636; No rows affected (0.013 seconds) # Query: 0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG' and `_hoodie_commit_time` > '20180924064636'; +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924070031 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ exit exit Read Optimized and Realtime Views for MOR with Spark-SQL after compaction docker exec -it adhoc-1 /bin/bash bash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1 --packages com.databricks:spark-avro_2.11:4.0.0 # Read Optimized View scala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false) +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ 1 row selected (1.6 seconds) scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor where symbol = 'GOOG'\").show(100, false) +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924064636 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924070031 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ # Realtime View scala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false) +---------+----------------------+--+ | symbol | _c1 | +---------+----------------------+--+ | GOOG | 2018-08-31 10:59:00 | +---------+----------------------+--+ scala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close from stock_ticks_mor_rt where symbol = 'GOOG'\").show(100, false) +----------------------+---------+----------------------+---------+------------+-----------+--+ | _hoodie_commit_time | symbol | ts | volume | open | close | +----------------------+---------+----------------------+---------+------------+-----------+--+ | 20180924064636 | GOOG | 2018-08-31 09:59:00 | 6330 | 1230.5 | 1230.02 | | 20180924070031 | GOOG | 2018-08-31 10:59:00 | 9021 | 1227.1993 | 1227.215 | +----------------------+---------+----------------------+---------+------------+-----------+--+ This brings the demo to an end. Testing Hudi in Local Docker environment You can bring up a hadoop docker environment containing Hadoop, Hive and Spark services with support for hudi. $ mvn pre-integration-test -DskipTests The above command builds docker images for all the services with current Hudi source installed at /var/hoodie/ws and also brings up the services using a compose file. We currently use Hadoop (v2.8.4), Hive (v2.3.3) and Spark (v2.3.1) in docker images. To bring down the containers $ cd hudi-integ-test $ mvn docker-compose:down If you want to bring up the docker containers, use $ cd hudi-integ-test $ mvn docker-compose:up -DdetachedMode=true Hudi is a library that is operated in a broader data analytics/ingestion environment involving Hadoop, Hive and Spark. Interoperability with all these systems is a key objective for us. We are actively adding integration-tests under hudi-integ-test/src/test/java that makes use of this docker environment (See hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieSanity.java ) Building Local Docker Containers: The docker images required for demo and running integration test are already in docker-hub. The docker images and compose scripts are carefully implemented so that they serve dual-purpose The docker images have inbuilt hudi jar files with environment variable pointing to those jars (HUDI_HADOOP_BUNDLE, ...) For running integration-tests, we need the jars generated locally to be used for running services within docker. The docker-compose scripts (see docker/compose/docker-compose_hadoop284_hive233_spark231.yml) ensures local jars override inbuilt jars by mounting local HUDI workspace over the docker location This helps avoid maintaining separate docker images and avoids the costly step of building HUDI docker images locally. But if users want to test hudi from locations with lower network bandwidth, they can still build local images run the script docker/build_local_docker_images.sh to build local docker images before running docker/setup_demo.sh Here are the commands: cd docker ./build_local_docker_images.sh ..... [INFO] Reactor Summary: [INFO] [INFO] hoodie ............................................. SUCCESS [ 1.709 s] [INFO] hudi-common ...................................... SUCCESS [ 9.015 s] [INFO] hudi-hadoop-mr ................................... SUCCESS [ 1.108 s] [INFO] hudi-client ...................................... SUCCESS [ 4.409 s] [INFO] hudi-hive ........................................ SUCCESS [ 0.976 s] [INFO] hudi-spark ....................................... SUCCESS [ 26.522 s] [INFO] hudi-utilities ................................... SUCCESS [ 16.256 s] [INFO] hudi-cli ......................................... SUCCESS [ 11.341 s] [INFO] hudi-hadoop-mr-bundle ............................ SUCCESS [ 1.893 s] [INFO] hudi-hive-bundle ................................. SUCCESS [ 14.099 s] [INFO] hudi-spark-bundle ................................ SUCCESS [ 58.252 s] [INFO] hudi-hadoop-docker ............................... SUCCESS [ 0.612 s] [INFO] hudi-hadoop-base-docker .......................... SUCCESS [04:04 min] [INFO] hudi-hadoop-namenode-docker ...................... SUCCESS [ 6.142 s] [INFO] hudi-hadoop-datanode-docker ...................... SUCCESS [ 7.763 s] [INFO] hudi-hadoop-history-docker ....................... SUCCESS [ 5.922 s] [INFO] hudi-hadoop-hive-docker .......................... SUCCESS [ 56.152 s] [INFO] hudi-hadoop-sparkbase-docker ..................... SUCCESS [01:18 min] [INFO] hudi-hadoop-sparkmaster-docker ................... SUCCESS [ 2.964 s] [INFO] hudi-hadoop-sparkworker-docker ................... SUCCESS [ 3.032 s] [INFO] hudi-hadoop-sparkadhoc-docker .................... SUCCESS [ 2.764 s] [INFO] hudi-integ-test .................................. SUCCESS [ 1.785 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 09:15 min [INFO] Finished at: 2018-09-10T17:47:37-07:00 [INFO] Final Memory: 236M/1848M [INFO] ------------------------------------------------------------------------ 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"concepts.html":{"url":"concepts.html","title":"概念","keywords":"","body":"Apache Hudi(发音为“Hudi”)在DFS的数据集上提供以下流原语 插入更新 (如何改变数据集?) 增量拉取 (如何获取变更的数据?) 在本节中，我们将讨论重要的概念和术语，这些概念和术语有助于理解并有效使用这些原语。 时间轴 在它的核心，Hudi维护一条包含在不同的即时时间所有对数据集操作的时间轴，从而提供，从不同时间点出发得到不同的视图下的数据集。Hudi即时包含以下组件 操作类型 : 对数据集执行的操作类型 即时时间 : 即时时间通常是一个时间戳(例如：20190117010349)，该时间戳按操作开始时间的顺序单调增加。 状态 : 即时的状态 Hudi保证在时间轴上执行的操作的原子性和基于即时时间的时间轴一致性。 执行的关键操作包括 COMMITS - 一次提交表示将一组记录原子写入到数据集中。 CLEANS - 删除数据集中不再需要的旧文件版本的后台活动。 DELTA_COMMIT - 增量提交是指将一批记录原子写入到MergeOnRead存储类型的数据集中，其中一些/所有数据都可以只写到增量日志中。 COMPACTION - 协调Hudi中差异数据结构的后台活动，例如：将更新从基于行的日志文件变成列格式。在内部，压缩表现为时间轴上的特殊提交。 ROLLBACK - 表示提交/增量提交不成功且已回滚，删除在写入过程中产生的所有部分文件。 SAVEPOINT - 将某些文件组标记为\"已保存\"，以便清理程序不会将其删除。在发生灾难/数据恢复的情况下，它有助于将数据集还原到时间轴上的某个点。 任何给定的即时都可以处于以下状态之一 REQUESTED - 表示已调度但尚未启动的操作。 INFLIGHT - 表示当前正在执行该操作。 COMPLETED - 表示在时间轴上完成了该操作。 上面的示例显示了在Hudi数据集上大约10:00到10:20之间发生的更新事件，大约每5分钟一次，将提交元数据以及其他后台清理/压缩保留在Hudi时间轴上。 观察的关键点是：提交时间指示数据的到达时间（上午10:20），而实际数据组织则反映了实际时间或事件时间，即数据所反映的（从07:00开始的每小时时段）。在权衡数据延迟和完整性时，这是两个关键概念。 如果有延迟到达的数据（事件时间为9:00的数据在10:20达到，延迟 >1 小时），我们可以看到upsert将新数据生成到更旧的时间段/文件夹中。 在时间轴的帮助下，增量查询可以只提取10:00以后成功提交的新数据，并非常高效地只消费更改过的文件，且无需扫描更大的文件范围，例如07:00后的所有时间段。 文件组织 Hudi将DFS上的数据集组织到基本路径下的目录结构中。数据集分为多个分区，这些分区是包含该分区的数据文件的文件夹，这与Hive表非常相似。 每个分区被相对于基本路径的特定分区路径区分开来。 在每个分区内，文件被组织为文件组，由文件id唯一标识。 每个文件组包含多个文件切片，其中每个切片包含在某个提交/压缩即时时间生成的基本列文件（*.parquet）以及一组日志文件（*.log*），该文件包含自生成基本文件以来对基本文件的插入/更新。 Hudi采用MVCC设计，其中压缩操作将日志和基本文件合并以产生新的文件片，而清理操作则将未使用的/较旧的文件片删除以回收DFS上的空间。 Hudi通过索引机制将给定的hoodie键（记录键+分区路径）映射到文件组，从而提供了高效的Upsert。 一旦将记录的第一个版本写入文件，记录键和文件组/文件id之间的映射就永远不会改变。 简而言之，映射的文件组包含一组记录的所有版本。 存储类型和视图 Hudi存储类型定义了如何在DFS上对数据进行索引和布局以及如何在这种组织之上实现上述原语和时间轴活动（即如何写入数据）。 反过来，视图定义了基础数据如何暴露给查询（即如何读取数据）。 存储类型 支持的视图 写时复制 读优化 + 增量 读时合并 读优化 + 增量 + 近实时 存储类型 Hudi支持以下存储类型。 写时复制 : 仅使用列文件格式（例如parquet）存储数据。通过在写入过程中执行同步合并以更新版本并重写文件。 读时合并 : 使用列式（例如parquet）+ 基于行（例如avro）的文件格式组合来存储数据。 更新记录到增量文件中，然后进行同步或异步压缩以生成列文件的新版本。 下表总结了这两种存储类型之间的权衡 权衡 写时复制 读时合并 数据延迟 更高 更低 更新代价(I/O) 更高（重写整个parquet文件） 更低（追加到增量日志） Parquet文件大小 更小（高更新代价（I/o）） 更大（低更新代价） 写放大 更高 更低（取决于压缩策略） 视图 Hudi支持以下存储数据的视图 读优化视图 : 在此视图上的查询将查看给定提交或压缩操作中数据集的最新快照。 该视图仅将最新文件切片中的基本/列文件暴露给查询，并保证与非Hudi列式数据集相比，具有相同的列式查询性能。 增量视图 : 对该视图的查询只能看到从某个提交/压缩后写入数据集的新数据。该视图有效地提供了更改流，来支持增量数据管道。 实时视图 : 在此视图上的查询将查看某个增量提交操作中数据集的最新快照。该视图通过动态合并最新的基本文件(例如parquet)和增量文件(例如avro)来提供近实时数据集（几分钟的延迟）。 下表总结了不同视图之间的权衡。 权衡 读优化 实时 数据延迟 更高 更低 查询延迟 更低（原始列式性能） 更高（合并列式 + 基于行的增量） 写时复制存储 写时复制存储中的文件片仅包含基本/列文件，并且每次提交都会生成新版本的基本文件。 换句话说，我们压缩每个提交，从而所有的数据都是以列数据的形式储存。在这种情况下，写入数据非常昂贵（我们需要重写整个列数据文件，即使只有一个字节的新数据被提交），而读取数据的成本则没有增加。 这种视图有利于读取繁重的分析工作。 以下内容说明了将数据写入写时复制存储并在其上运行两个查询时，它是如何工作的。 随着数据的写入，对现有文件组的更新将为该文件组生成一个带有提交即时时间标记的新切片，而插入分配一个新文件组并写入该文件组的第一个切片。 这些文件切片及其提交即时时间在上面用颜色编码。 针对这样的数据集运行SQL查询（例如：select count(*)统计该分区中的记录数目），首先检查时间轴上的最新提交并过滤每个文件组中除最新文件片以外的所有文件片。 如您所见，旧查询不会看到以粉红色标记的当前进行中的提交的文件，但是在该提交后的新查询会获取新数据。因此，查询不受任何写入失败/部分写入的影响，仅运行在已提交数据上。 写时复制存储的目的是从根本上改善当前管理数据集的方式，通过以下方法来实现 优先支持在文件级原子更新数据，而无需重写整个表/分区 能够只读取更新的部分，而不是进行低效的扫描或搜索 严格控制文件大小来保持出色的查询性能（小的文件会严重损害查询性能）。 读时合并存储 读时合并存储是写时复制的升级版，从某种意义上说，它仍然可以通过读优化表提供数据集的读取优化视图（写时复制的功能）。 此外，它将每个文件组的更新插入存储到基于行的增量日志中，通过文件id，将增量日志和最新版本的基本文件进行合并，从而提供近实时的数据查询。因此，此存储类型智能地平衡了读和写的成本，以提供近乎实时的查询。 这里最重要的一点是压缩器，它现在可以仔细挑选需要压缩到其列式基础文件中的增量日志（根据增量日志的文件大小），以保持查询性能（较大的增量日志将会提升近实时的查询时间，并同时需要更长的合并时间）。 以下内容说明了存储的工作方式，并显示了对近实时表和读优化表的查询。 此示例中发生了很多有趣的事情，这些带出了该方法的微妙之处。 现在，我们每1分钟左右就有一次提交，这是其他存储类型无法做到的。 现在，在每个文件id组中，都有一个增量日志，其中包含对基础列文件中记录的更新。 在示例中，增量日志包含10:05至10:10的所有数据。与以前一样，基本列式文件仍使用提交进行版本控制。 因此，如果只看一眼基本文件，那么存储布局看起来就像是写时复制表的副本。 定期压缩过程会从增量日志中合并这些更改，并生成基础文件的新版本，就像示例中10:05发生的情况一样。 有两种查询同一存储的方式：读优化（RO）表和近实时（RT）表，具体取决于我们选择查询性能还是数据新鲜度。 对于RO表来说，提交数据在何时可用于查询将有些许不同。 请注意，以10:10运行的（在RO表上的）此类查询将不会看到10:05之后的数据，而在RT表上的查询总会看到最新的数据。 何时触发压缩以及压缩什么是解决这些难题的关键。 通过实施压缩策略，在该策略中，与较旧的分区相比，我们会积极地压缩最新的分区，从而确保RO表能够以一致的方式看到几分钟内发布的数据。 读时合并存储上的目的是直接在DFS上启用近实时处理，而不是将数据复制到专用系统，后者可能无法处理大数据量。 该存储还有一些其他方面的好处，例如通过避免数据的同步合并来减少写放大，即批量数据中每1字节数据需要的写入数据量。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"writing_data.html":{"url":"writing_data.html","title":"写入数据","keywords":"","body":"这一节我们将介绍使用DeltaStreamer工具从外部源甚至其他Hudi数据集摄取新更改的方法， 以及通过使用Hudi数据源的upserts加快大型Spark作业的方法。 对于此类数据集，我们可以使用各种查询引擎查询它们。 写操作 在此之前，了解Hudi数据源及delta streamer工具提供的三种不同的写操作以及如何最佳利用它们可能会有所帮助。 这些操作可以在针对数据集发出的每个提交/增量提交中进行选择/更改。 UPSERT（插入更新） ：这是默认操作，在该操作中，通过查找索引，首先将输入记录标记为插入或更新。 在运行启发式方法以确定如何最好地将这些记录放到存储上，如优化文件大小之类后，这些记录最终会被写入。 对于诸如数据库更改捕获之类的用例，建议该操作，因为输入几乎肯定包含更新。 INSERT（插入） ：就使用启发式方法确定文件大小而言，此操作与插入更新（UPSERT）非常相似，但此操作完全跳过了索引查找步骤。 因此，对于日志重复数据删除等用例（结合下面提到的过滤重复项的选项），它可以比插入更新快得多。 插入也适用于这种用例，这种情况数据集可以允许重复项，但只需要Hudi的事务写/增量提取/存储管理功能。 BULK_INSERT（批插入） ：插入更新和插入操作都将输入记录保存在内存中，以加快存储优化启发式计算的速度（以及其它未提及的方面）。 所以对Hudi数据集进行初始加载/引导时这两种操作会很低效。批量插入提供与插入相同的语义，但同时实现了基于排序的数据写入算法， 该算法可以很好地扩展数百TB的初始负载。但是，相比于插入和插入更新能保证文件大小，批插入在调整文件大小上只能尽力而为。 DeltaStreamer HoodieDeltaStreamer实用工具 (hudi-utilities-bundle中的一部分) 提供了从DFS或Kafka等不同来源进行摄取的方式，并具有以下功能。 从Kafka单次摄取新事件，从Sqoop、HiveIncrementalPuller输出或DFS文件夹中的多个文件 增量导入 支持json、avro或自定义记录类型的传入数据 管理检查点，回滚和恢复 利用DFS或Confluent schema注册表的Avro模式。 支持自定义转换操作 命令行选项更详细地描述了这些功能： [hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` --help Usage: [options] Options: --commit-on-errors Commit even when some records failed to be written Default: false --enable-hive-sync Enable syncing to hive Default: false --filter-dupes Should duplicate records from source be dropped/filtered outbefore insert/bulk-insert Default: false --help, -h --hudi-conf Any configuration that can be set in the properties file (using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter Default: [] --op Takes one of these values : UPSERT (default), INSERT (use when input is purely new data/inserts to gain speed) Default: UPSERT Possible Values: [UPSERT, INSERT, BULK_INSERT] --payload-class subclass of HoodieRecordPayload, that works off a GenericRecord. Implement your own, if you want to do something other than overwriting existing value Default: org.apache.hudi.OverwriteWithLatestAvroPayload --props path to properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source. For Hudi client props, sane defaults are used, but recommend use to provide basic things like metrics endpoints, hive configs etc. For sources, referto individual classes, for supported properties. Default: file:///Users/vinoth/bin/hoodie/src/test/resources/delta-streamer-config/dfs-source.properties --schemaprovider-class subclass of org.apache.hudi.utilities.schema.SchemaProvider to attach schemas to input & target table data, built in options: FilebasedSchemaProvider Default: org.apache.hudi.utilities.schema.FilebasedSchemaProvider --source-class Subclass of org.apache.hudi.utilities.sources to read data. Built-in options: org.apache.hudi.utilities.sources.{JsonDFSSource (default), AvroDFSSource, JsonKafkaSource, AvroKafkaSource, HiveIncrPullSource} Default: org.apache.hudi.utilities.sources.JsonDFSSource --source-limit Maximum amount of data to read from source. Default: No limit For e.g: DFSSource => max bytes to read, KafkaSource => max events to read Default: 9223372036854775807 --source-ordering-field Field within source record to decide how to break ties between records with same key in input data. Default: 'ts' holding unix timestamp of record Default: ts --spark-master spark master to use. Default: local[2] * --target-base-path base path for the target Hudi dataset. (Will be created if did not exist first time around. If exists, expected to be a Hudi dataset) * --target-table name of the target table in Hive --transformer-class subclass of org.apache.hudi.utilities.transform.Transformer. UDF to transform raw source dataset to a target dataset (conforming to target schema) before writing. Default : Not set. E:g - org.apache.hudi.utilities.transform.SqlQueryBasedTransformer (which allows a SQL query template to be passed as a transformation function) 该工具采用层次结构组成的属性文件，并具有可插拔的接口，用于提取数据、生成密钥和提供模式。 从Kafka和DFS摄取数据的示例配置在这里：hudi-utilities/src/test/resources/delta-streamer-config。 例如：当您让Confluent Kafka、Schema注册表启动并运行后，可以用这个命令产生一些测试数据 （impressions.avro， 由schema-registry代码库提供） [confluent-5.0.0]$ bin/ksql-datagen schema=../impressions.avro format=avro topic=impressions key=impressionid 然后用如下命令摄取这些数据。 [hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\ --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\ --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\ --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\ --source-ordering-field impresssiontime \\ --target-base-path file:///tmp/hudi-deltastreamer-op --target-table uber.impressions \\ --op BULK_INSERT 在某些情况下，您可能需要预先将现有数据集迁移到Hudi。 请参考迁移指南。 Datasource Writer hudi-spark模块提供了DataSource API，可以将任何DataFrame写入（也可以读取）到Hudi数据集中。 以下是在指定需要使用的字段名称的之后，如何插入更新DataFrame的方法，这些字段包括 recordKey => _row_key、partitionPath => partition和precombineKey => timestamp inputDF.write() .format(\"org.apache.hudi\") .options(clientOpts) // 可以传入任何Hudi客户端参数 .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\") .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\") .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\") .option(HoodieWriteConfig.TABLE_NAME, tableName) .mode(SaveMode.Append) .save(basePath); 与Hive同步 上面的两个工具都支持将数据集的最新模式同步到Hive Metastore，以便查询新的列和分区。 如果需要从命令行或在独立的JVM中运行它，Hudi提供了一个HiveSyncTool， 在构建了hudi-hive模块之后，可以按以下方式调用它。 cd hudi-hive ./run_sync_tool.sh [hudi-hive]$ ./run_sync_tool.sh --help Usage: [options] Options: * --base-path Basepath of Hudi dataset to sync * --database name of the target database in Hive --help, -h Default: false * --jdbc-url Hive jdbc connect url * --pass Hive password * --table name of the target table in Hive * --user Hive username 删除数据 通过允许用户指定不同的数据记录负载实现，Hudi支持对存储在Hudi数据集中的数据执行两种类型的删除。 Soft Deletes（软删除） ：使用软删除时，用户希望保留键，但仅使所有其他字段的值都为空。 通过确保适当的字段在数据集模式中可以为空，并在将这些字段设置为null之后直接向数据集插入更新这些记录，即可轻松实现这一点。 Hard Deletes（硬删除） ：这种更强形式的删除是从数据集中彻底删除记录在存储上的任何痕迹。 这可以通过触发一个带有自定义负载实现的插入更新来实现，这种实现可以使用总是返回Optional.Empty作为组合值的DataSource或DeltaStreamer。 Hudi附带了一个内置的org.apache.hudi.EmptyHoodieRecordPayload类，它就是实现了这一功能。 deleteDF // 仅包含要删除的记录的DataFrame .write().format(\"org.apache.hudi\") .option(...) // 根据设置需要添加HUDI参数，例如记录键、分区路径和其他参数 // 指定record_key，partition_key，precombine_fieldkey和常规参数 .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.EmptyHoodieRecordPayload\") 存储管理 Hudi还对存储在Hudi数据集中的数据执行几个关键的存储管理功能。在DFS上存储数据的关键方面是管理文件大小和数量以及回收存储空间。 例如，HDFS在处理小文件上性能很差，这会对Name Node的内存及RPC施加很大的压力，并可能破坏整个集群的稳定性。 通常，查询引擎可在较大的列文件上提供更好的性能，因为它们可以有效地摊销获得列统计信息等的成本。 即使在某些云数据存储上，列出具有大量小文件的目录也常常比较慢。 以下是一些有效管理Hudi数据集存储的方法。 Hudi中的小文件处理功能，可以分析传入的工作负载并将插入内容分配到现有文件组中， 而不是创建新文件组。新文件组会生成小文件。 可以配置Cleaner来清理较旧的文件片，清理的程度可以调整， 具体取决于查询所需的最长时间和增量拉取所需的回溯。 用户还可以调整基础/parquet文件、日志文件的大小 和预期的压缩率，使足够数量的插入被分到同一个文件组中，最终产生大小合适的基础文件。 智能调整批插入并行度，可以产生大小合适的初始文件组。 实际上，正确执行此操作非常关键，因为文件组一旦创建后就不能删除，只能如前所述对其进行扩展。 对于具有大量更新的工作负载，读取时合并存储提供了一种很好的机制， 可以快速将其摄取到较小的文件中，之后通过压缩将它们合并为较大的基础文件。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"querying_data.html":{"url":"querying_data.html","title":"查询数据","keywords":"","body":"从概念上讲，Hudi物理存储一次数据到DFS上，同时在其上提供三个逻辑视图，如之前所述。 数据集同步到Hive Metastore后，它将提供由Hudi的自定义输入格式支持的Hive外部表。一旦提供了适当的Hudi捆绑包， 就可以通过Hive、Spark和Presto之类的常用查询引擎来查询数据集。 具体来说，在写入过程中传递了两个由table name命名的Hive表。 例如，如果table name = hudi_tbl，我们得到 hudi_tbl 实现了由 HoodieParquetInputFormat 支持的数据集的读优化视图，从而提供了纯列式数据。 hudi_tbl_rt 实现了由 HoodieParquetRealtimeInputFormat 支持的数据集的实时视图，从而提供了基础数据和日志数据的合并视图。 如概念部分所述，增量处理所需要的 一个关键原语是增量拉取（以从数据集中获取更改流/日志）。您可以增量提取Hudi数据集，这意味着自指定的即时时间起， 您可以只获得全部更新和新行。 这与插入更新一起使用，对于构建某些数据管道尤其有用，包括将1个或多个源Hudi表（数据流/事实）以增量方式拉出（流/事实） 并与其他表（数据集/维度）结合以写出增量到目标Hudi数据集。增量视图是通过查询上表之一实现的，并具有特殊配置， 该特殊配置指示查询计划仅需要从数据集中获取增量数据。 接下来，我们将详细讨论在每个查询引擎上如何访问所有三个视图。 Hive 为了使Hive能够识别Hudi数据集并正确查询， HiveServer2需要在其辅助jars路径中提供hudi-hadoop-mr-bundle-x.y.z-SNAPSHOT.jar。 这将确保输入格式类及其依赖项可用于查询计划和执行。 读优化表 除了上述设置之外，对于beeline cli访问，还需要将hive.input.format变量设置为org.apache.hudi.hadoop.HoodieParquetInputFormat输入格式的完全限定路径名。 对于Tez，还需要将hive.tez.input.format设置为org.apache.hadoop.hive.ql.io.HiveInputFormat。 实时表 除了在HiveServer2上安装Hive捆绑jars之外，还需要将其放在整个集群的hadoop/hive安装中，这样查询也可以使用自定义RecordReader。 增量拉取 HiveIncrementalPuller允许通过HiveQL从大型事实/维表中增量提取更改， 结合了Hive（可靠地处理复杂的SQL查询）和增量原语的好处（通过增量拉取而不是完全扫描来加快查询速度）。 该工具使用Hive JDBC运行hive查询并将其结果保存在临时表中，这个表可以被插入更新。 Upsert实用程序（HoodieDeltaStreamer）具有目录结构所需的所有状态，以了解目标表上的提交时间应为多少。 例如：/app/incremental-hql/intermediate/{source_table_name}_temp/{last_commit_included}。 已注册的Delta Hive表的格式为{tmpdb}.{source_table}_{last_commit_included}。 以下是HiveIncrementalPuller的配置选项 | 配置 | 描述 | 默认值 | |hiveUrl| 要连接的Hive Server 2的URL | | |hiveUser| Hive Server 2 用户名 | | |hivePass| Hive Server 2 密码 | | |queue| YARN 队列名称 | | |tmp| DFS中存储临时增量数据的目录。目录结构将遵循约定。请参阅以下部分。 | | |extractSQLFile| 在源表上要执行的提取数据的SQL。提取的数据将是自特定时间点以来已更改的所有行。 | | |sourceTable| 源表名称。在Hive环境属性中需要设置。 | | |targetTable| 目标表名称。中间存储目录结构需要。 | | |sourceDataPath| 源DFS基本路径。这是读取Hudi元数据的地方。 | | |targetDataPath| 目标DFS基本路径。 这是计算fromCommitTime所必需的。 如果显式指定了fromCommitTime，则不需要设置这个参数。 | | |tmpdb| 用来创建中间临时增量表的数据库 | hoodie_temp | |fromCommitTime| 这是最重要的参数。 这是从中提取更改的记录的时间点。 | | |maxCommits| 要包含在拉取中的提交数。将此设置为-1将包括从fromCommitTime开始的所有提交。将此设置为大于0的值，将包括在fromCommitTime之后仅更改指定提交次数的记录。如果您需要一次赶上两次提交，则可能需要这样做。| 3 | |help| 实用程序帮助 | | 设置fromCommitTime=0和maxCommits=-1将提取整个源数据集，可用于启动Backfill。 如果目标数据集是Hudi数据集，则该实用程序可以确定目标数据集是否没有提交或延迟超过24小时（这是可配置的）， 它将自动使用Backfill配置，因为增量应用最近24小时的更改会比Backfill花费更多的时间。 该工具当前的局限性在于缺乏在混合模式（正常模式和增量模式）下自联接同一表的支持。 关于使用Fetch任务执行的Hive查询的说明： 由于Fetch任务为每个分区调用InputFormat.listStatus()，每个listStatus()调用都会列出Hoodie元数据。 为了避免这种情况，如下操作可能是有用的，即使用Hive session属性对增量查询禁用Fetch任务： set hive.fetch.task.conversion = none;。这将确保Hive查询使用Map Reduce执行， 合并分区（用逗号分隔），并且对所有这些分区仅调用一次InputFormat.listStatus()。 Spark Spark可将Hudi jars和捆绑包轻松部署和管理到作业/笔记本中。简而言之，通过Spark有两种方法可以访问Hudi数据集。 Hudi DataSource：支持读取优化和增量拉取，类似于标准数据源（例如：spark.read.parquet）的工作方式。 以Hive表读取：支持所有三个视图，包括实时视图，依赖于自定义的Hudi输入格式（再次类似Hive）。 通常，您的spark作业需要依赖hudi-spark或hudi-spark-bundle-x.y.z.jar， 它们必须位于驱动程序和执行程序的类路径上（提示：使用--jars参数）。 读优化表 要使用SparkSQL将RO表读取为Hive表，只需按如下所示将路径过滤器推入sparkContext。 对于Hudi表，该方法保留了Spark内置的读取Parquet文件的优化功能，例如进行矢量化读取。 spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]); 如果您希望通过数据源在DFS上使用全局路径，则只需执行以下类似操作即可得到Spark DataFrame。 Dataset hoodieROViewDF = spark.read().format(\"org.apache.hudi\") // pass any path glob, can include hudi & non-hudi datasets .load(\"/glob/path/pattern\"); 实时表 当前，实时表只能在Spark中作为Hive表进行查询。为了做到这一点，设置spark.sql.hive.convertMetastoreParquet = false， 迫使Spark回退到使用Hive Serde读取数据（计划/执行仍然是Spark）。 $ spark-shell --jars hudi-spark-bundle-x.y.z-SNAPSHOT.jar --driver-class-path /etc/hive/conf --packages com.databricks:spark-avro_2.11:4.0.0 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g --master yarn-client scala> sqlContext.sql(\"select count(*) from hudi_rt where datestr = '2016-10-02'\").show() 增量拉取 hudi-spark模块提供了DataSource API，这是一种从Hudi数据集中提取数据并通过Spark处理数据的更优雅的方法。 如下所示是一个示例增量拉取，它将获取自beginInstantTime以来写入的所有记录。 Dataset hoodieIncViewDF = spark.read() .format(\"org.apache.hudi\") .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(), DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL()) .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), ) .load(tablePath); // For incremental view, pass in the root/base path of dataset 请参阅设置部分，以查看所有数据源选项。 另外，HoodieReadClient通过Hudi的隐式索引提供了以下功能。 | API | 描述 | | read(keys) | 使用Hudi自己的索通过快速查找将与键对应的数据作为DataFrame读出 | | filterExists() | 从提供的RDD[HoodieRecord]中过滤出已经存在的记录。对删除重复数据有用 | | checkExists(keys) | 检查提供的键是否存在于Hudi数据集中 | Presto Presto是一种常用的查询引擎，可提供交互式查询性能。 Hudi RO表可以在Presto中无缝查询。 这需要在整个安装过程中将hudi-presto-bundle jar放入/plugin/hive-hadoop2/中。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"configurations.html":{"url":"configurations.html","title":"配置","keywords":"","body":"该页面介绍了几种配置写入或读取Hudi数据集的作业的方法。 简而言之，您可以在几个级别上控制行为。 Spark数据源配置 : 这些配置控制Hudi Spark数据源，提供如下功能： 定义键和分区、选择写操作、指定如何合并记录或选择要读取的视图类型。 WriteClient 配置 : 在内部，Hudi数据源使用基于RDD的HoodieWriteClient API 真正执行对存储的写入。 这些配置可对文件大小、压缩（compression）、并行度、压缩（compaction）、写入模式、清理等底层方面进行完全控制。 尽管Hudi提供了合理的默认设置，但在不同情形下，可能需要对这些配置进行调整以针对特定的工作负载进行优化。 RecordPayload 配置 : 这是Hudi提供的最底层的定制。 RecordPayload定义了如何根据传入的新记录和存储的旧记录来产生新值以进行插入更新。 Hudi提供了诸如OverwriteWithLatestAvroPayload的默认实现，该实现仅使用最新或最后写入的记录来更新存储。 在数据源和WriteClient级别，都可以将其重写为扩展HoodieRecordPayload类的自定义类。 与云存储连接 无论使用RDD/WriteClient API还是数据源，以下信息都有助于配置对云存储的访问。 AWS S3 S3和Hudi协同工作所需的配置。 Google Cloud Storage GCS和Hudi协同工作所需的配置。 Spark数据源配置 可以通过将以下选项传递到option(k,v)方法中来配置使用数据源的Spark作业。 实际的数据源级别配置在下面列出。 写选项 另外，您可以使用options()或option(k,v)方法直接传递任何WriteClient级别的配置。 inputDF.write() .format(\"org.apache.hudi\") .options(clientOpts) // 任何Hudi客户端选项都可以传入 .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\") .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\") .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\") .option(HoodieWriteConfig.TABLE_NAME, tableName) .mode(SaveMode.Append) .save(basePath); 用于通过write.format.option(...)写入数据集的选项 TABLE_NAME_OPT_KEY 属性：hoodie.datasource.write.table.name [必须] Hive表名，用于将数据集注册到其中。 OPERATION_OPT_KEY 属性：hoodie.datasource.write.operation, 默认值：upsert 是否为写操作进行插入更新、插入或批量插入。使用bulkinsert将新数据加载到表中，之后使用upsert或insert。 批量插入使用基于磁盘的写入路径来扩展以加载大量输入，而无需对其进行缓存。 STORAGE_TYPE_OPT_KEY 属性：hoodie.datasource.write.storage.type, 默认值：COPY_ON_WRITE 此写入的基础数据的存储类型。两次写入之间不能改变。 PRECOMBINE_FIELD_OPT_KEY 属性：hoodie.datasource.write.precombine.field, 默认值：ts 实际写入之前在preCombining中使用的字段。 当两个记录具有相同的键值时，我们将使用Object.compareTo(..)从precombine字段中选择一个值最大的记录。 PAYLOAD_CLASS_OPT_KEY 属性：hoodie.datasource.write.payload.class, 默认值：org.apache.hudi.OverwriteWithLatestAvroPayload 使用的有效载荷类。如果您想在插入更新或插入时使用自己的合并逻辑，请重写此方法。 这将使得PRECOMBINE_FIELD_OPT_VAL设置的任何值无效 RECORDKEY_FIELD_OPT_KEY 属性：hoodie.datasource.write.recordkey.field, 默认值：uuid 记录键字段。用作HoodieKey中recordKey部分的值。 实际值将通过在字段值上调用.toString()来获得。可以使用点符号指定嵌套字段，例如：a.b.c PARTITIONPATH_FIELD_OPT_KEY 属性：hoodie.datasource.write.partitionpath.field, 默认值：partitionpath 分区路径字段。用作HoodieKey中partitionPath部分的值。 通过调用.toString()获得实际的值 KEYGENERATOR_CLASS_OPT_KEY 属性：hoodie.datasource.write.keygenerator.class, 默认值：org.apache.hudi.SimpleKeyGenerator 键生成器类，实现从输入的Row对象中提取键 COMMIT_METADATA_KEYPREFIX_OPT_KEY 属性：hoodie.datasource.write.commitmeta.key.prefix, 默认值：_ 以该前缀开头的选项键会自动添加到提交/增量提交的元数据中。 这对于与hudi时间轴一致的方式存储检查点信息很有用 INSERT_DROP_DUPS_OPT_KEY 属性：hoodie.datasource.write.insert.drop.duplicates, 默认值：false 如果设置为true，则在插入操作期间从传入DataFrame中过滤掉所有重复记录。 HIVE_SYNC_ENABLED_OPT_KEY 属性：hoodie.datasource.hive_sync.enable, 默认值：false 设置为true时，将数据集注册并同步到Apache Hive Metastore HIVE_DATABASE_OPT_KEY 属性：hoodie.datasource.hive_sync.database, 默认值：default 要同步到的数据库 HIVE_TABLE_OPT_KEY 属性：hoodie.datasource.hive_sync.table, [Required] 要同步到的表 HIVE_USER_OPT_KEY 属性：hoodie.datasource.hive_sync.username, 默认值：hive 要使用的Hive用户名 HIVE_PASS_OPT_KEY 属性：hoodie.datasource.hive_sync.password, 默认值：hive 要使用的Hive密码 HIVE_URL_OPT_KEY 属性：hoodie.datasource.hive_sync.jdbcurl, 默认值：jdbc:hive2://localhost:10000 Hive metastore url HIVE_PARTITION_FIELDS_OPT_KEY 属性：hoodie.datasource.hive_sync.partition_fields, 默认值： 数据集中用于确定Hive分区的字段。 HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY 属性：hoodie.datasource.hive_sync.partition_extractor_class, 默认值：org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor 用于将分区字段值提取到Hive分区列中的类。 HIVE_ASSUME_DATE_PARTITION_OPT_KEY 属性：hoodie.datasource.hive_sync.assume_date_partitioning, 默认值：false 假设分区格式是yyyy/mm/dd 读选项 用于通过read.format.option(...)读取数据集的选项 VIEW_TYPE_OPT_KEY 属性：hoodie.datasource.view.type, 默认值：read_optimized 是否需要以某种模式读取数据，增量模式（自InstantTime以来的新数据） （或）读优化模式（基于列数据获取最新视图） （或）实时模式（基于行和列数据获取最新视图） BEGIN_INSTANTTIME_OPT_KEY 属性：hoodie.datasource.read.begin.instanttime, [在增量模式下必须] 开始增量提取数据的即时时间。这里的instanttime不必一定与时间轴上的即时相对应。 取出以instant_time > BEGIN_INSTANTTIME写入的新数据。 例如：'20170901080000'将获取2017年9月1日08:00 AM之后写入的所有新数据。 END_INSTANTTIME_OPT_KEY 属性：hoodie.datasource.read.end.instanttime, 默认值：最新即时（即从开始即时获取所有新数据） 限制增量提取的数据的即时时间。取出以instant_time 写入的新数据。 WriteClient 配置 直接使用RDD级别api进行编程的Jobs可以构建一个HoodieWriteConfig对象，并将其传递给HoodieWriteClient构造函数。 HoodieWriteConfig可以使用以下构建器模式构建。 HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder() .withPath(basePath) .forTable(tableName) .withSchema(schemaStr) .withProps(props) // 从属性文件传递原始k、v对。 .withCompactionConfig(HoodieCompactionConfig.newBuilder().withXXX(...).build()) .withIndexConfig(HoodieIndexConfig.newBuilder().withXXX(...).build()) ... .build(); 以下各节介绍了写配置的不同方面，并解释了最重要的配置及其属性名称和默认值。 withPath(hoodie_base_path) 属性：hoodie.base.path [必须] 创建所有数据分区所依据的基本DFS路径。 始终在前缀中明确指明存储方式（例如hdfs://，s3://等）。 Hudi将有关提交、保存点、清理审核日志等的所有主要元数据存储在基本目录下的.hoodie目录中。 withSchema(schema_str) 属性：hoodie.avro.schema [必须] 这是数据集的当前读取器的avro模式（schema）。 这是整个模式的字符串。HoodieWriteClient使用此模式传递到HoodieRecordPayload的实现，以从源格式转换为avro记录。 在更新过程中重写记录时也使用此模式。 forTable(table_name) 属性：hoodie.table.name [必须] 数据集的表名，将用于在Hive中注册。每次运行需要相同。 withBulkInsertParallelism(bulk_insert_parallelism = 1500) 属性：hoodie.bulkinsert.shuffle.parallelism 批量插入旨在用于较大的初始导入，而此处的并行度决定了数据集中文件的初始数量。 调整此值以达到在初始导入期间所需的最佳尺寸。 withParallelism(insert_shuffle_parallelism = 1500, upsert_shuffle_parallelism = 1500) 属性：hoodie.insert.shuffle.parallelism, hoodie.upsert.shuffle.parallelism 最初导入数据后，此并行度将控制用于读取输入记录的初始并行度。 确保此值足够高，例如：1个分区用于1 GB的输入数据 combineInput(on_insert = false, on_update=true) 属性：hoodie.combine.before.insert, hoodie.combine.before.upsert 在DFS中插入或更新之前先组合输入RDD并将多个部分记录合并为单个记录的标志 withWriteStatusStorageLevel(level = MEMORY_AND_DISK_SER) 属性：hoodie.write.status.storage.level HoodieWriteClient.insert和HoodieWriteClient.upsert返回一个持久的RDD[WriteStatus]， 这是因为客户端可以选择检查WriteStatus并根据失败选择是否提交。这是此RDD的存储级别的配置 withAutoCommit(autoCommit = true) 属性：hoodie.auto.commit 插入和插入更新后，HoodieWriteClient是否应该自动提交。 客户端可以选择关闭自动提交，并在\"定义的成功条件\"下提交 withAssumeDatePartitioning(assumeDatePartitioning = false) 属性：hoodie.assume.date.partitioning HoodieWriteClient是否应该假设数据按日期划分，即从基本路径划分为三个级别。 这是支持 withConsistencyCheckEnabled(enabled = false) 属性：hoodie.consistency.check.enabled HoodieWriteClient是否应该执行其他检查，以确保写入的文件在基础文件系统/存储上可列出。 将其设置为true可以解决S3的最终一致性模型，并确保作为提交的一部分写入的所有数据均能准确地用于查询。 索引配置 以下配置控制索引行为，该行为将传入记录标记为对较旧记录的插入或更新。 withIndexConfig (HoodieIndexConfig) 可插入以具有外部索引（HBase）或使用存储在Parquet文件中的默认布隆过滤器（bloom filter） withIndexType(indexType = BLOOM) 属性：hoodie.index.type 要使用的索引类型。默认为布隆过滤器。可能的选项是[BLOOM | HBASE | INMEMORY]。 布隆过滤器消除了对外部系统的依赖，并存储在Parquet数据文件的页脚中 bloomFilterNumEntries(numEntries = 60000) 属性：hoodie.index.bloom.num_entries 仅在索引类型为BLOOM时适用。这是要存储在布隆过滤器中的条目数。 我们假设maxParquetFileSize为128MB，averageRecordSize为1024B，因此，一个文件中的记录总数约为130K。 默认值（60000）大约是此近似值的一半。HUDI-56 描述了如何动态地对此进行计算。 警告：将此值设置得太低，将产生很多误报，并且索引查找将必须扫描比其所需的更多的文件；如果将其设置得非常高，将线性增加每个数据文件的大小（每50000个条目大约4KB）。 bloomFilterFPP(fpp = 0.000000001) 属性：hoodie.index.bloom.fpp 仅在索引类型为BLOOM时适用。根据条目数允许的错误率。 这用于计算应为布隆过滤器分配多少位以及哈希函数的数量。通常将此值设置得很低（默认值：0.000000001），我们希望在磁盘空间上进行权衡以降低误报率 bloomIndexPruneByRanges(pruneRanges = true) 属性：hoodie.bloom.index.prune.by.ranges 仅在索引类型为BLOOM时适用。为true时，从文件框定信息，可以加快索引查找的速度。 如果键具有单调递增的前缀，例如时间戳，则特别有用。 bloomIndexUseCaching(useCaching = true) 属性：hoodie.bloom.index.use.caching 仅在索引类型为BLOOM时适用。为true时，将通过减少用于计算并行度或受影响分区的IO来缓存输入的RDD以加快索引查找 bloomIndexTreebasedFilter(useTreeFilter = true) 属性：hoodie.bloom.index.use.treebased.filter 仅在索引类型为BLOOM时适用。为true时，启用基于间隔树的文件过滤优化。与暴力模式相比，此模式可根据键范围加快文件过滤速度 bloomIndexBucketizedChecking(bucketizedChecking = true) 属性：hoodie.bloom.index.bucketized.checking 仅在索引类型为BLOOM时适用。为true时，启用了桶式布隆过滤。这减少了在基于排序的布隆索引查找中看到的偏差 bloomIndexKeysPerBucket(keysPerBucket = 10000000) 属性：hoodie.bloom.index.keys.per.bucket 仅在启用bloomIndexBucketizedChecking并且索引类型为bloom的情况下适用。 此配置控制“存储桶”的大小，该大小可跟踪对单个文件进行的记录键检查的次数，并且是分配给执行布隆过滤器查找的每个分区的工作单位。 较高的值将分摊将布隆过滤器读取到内存的固定成本。 bloomIndexParallelism(0) 属性：hoodie.bloom.index.parallelism 仅在索引类型为BLOOM时适用。这是索引查找的并行度，其中涉及Spark Shuffle。 默认情况下，这是根据输入的工作负载特征自动计算的 hbaseZkQuorum(zkString) [必须] 属性：hoodie.index.hbase.zkquorum 仅在索引类型为HBASE时适用。要连接的HBase ZK Quorum URL。 hbaseZkPort(port) [必须] 属性：hoodie.index.hbase.zkport 仅在索引类型为HBASE时适用。要连接的HBase ZK Quorum端口。 hbaseZkZnodeParent(zkZnodeParent) [必须] 属性：hoodie.index.hbase.zknode.path 仅在索引类型为HBASE时适用。这是根znode，它将包含HBase创建及使用的所有znode。 hbaseTableName(tableName) [必须] 属性：hoodie.index.hbase.table 仅在索引类型为HBASE时适用。HBase表名称，用作索引。Hudi将row_key和[partition_path, fileID, commitTime]映射存储在表中。 存储选项 控制有关调整parquet和日志文件大小的方面。 withStorageConfig (HoodieStorageConfig) limitFileSize (size = 120MB) 属性：hoodie.parquet.max.file.size Hudi写阶段生成的parquet文件的目标大小。对于DFS，这需要与基础文件系统块大小保持一致，以实现最佳性能。 parquetBlockSize(rowgroupsize = 120MB) 属性：hoodie.parquet.block.size Parquet行组大小。最好与文件大小相同，以便将文件中的单个列连续存储在磁盘上 parquetPageSize(pagesize = 1MB) 属性：hoodie.parquet.page.size Parquet页面大小。页面是parquet文件中的读取单位。 在一个块内，页面被分别压缩。 parquetCompressionRatio(parquetCompressionRatio = 0.1) 属性：hoodie.parquet.compression.ratio 当Hudi尝试调整新parquet文件的大小时，预期对parquet数据进行压缩的比例。 如果bulk_insert生成的文件小于预期大小，请增加此值 parquetCompressionCodec(parquetCompressionCodec = gzip) 属性：hoodie.parquet.compression.codec Parquet压缩编解码方式名称。默认值为gzip。可能的选项是[gzip | snappy | uncompressed | lzo] logFileMaxSize(logFileSize = 1GB) 属性：hoodie.logfile.max.size LogFile的最大大小。这是在将日志文件移到下一个版本之前允许的最大大小。 logFileDataBlockMaxSize(dataBlockSize = 256MB) 属性：hoodie.logfile.data.block.max.size LogFile数据块的最大大小。这是允许将单个数据块附加到日志文件的最大大小。 这有助于确保附加到日志文件的数据被分解为可调整大小的块，以防止发生OOM错误。此大小应大于JVM内存。 logFileToParquetCompressionRatio(logFileToParquetCompressionRatio = 0.35) 属性：hoodie.logfile.to.parquet.compression.ratio 随着记录从日志文件移动到parquet，预期会进行额外压缩的比例。 用于merge_on_read存储，以将插入内容发送到日志文件中并控制压缩parquet文件的大小。 parquetCompressionCodec(parquetCompressionCodec = gzip) 属性：hoodie.parquet.compression.codec Parquet文件的压缩编解码方式 压缩（Compaction）配置 压缩配置用于控制压缩（将日志文件合并到新的parquet基本文件中）、清理（回收较旧及未使用的文件组）。 withCompactionConfig (HoodieCompactionConfig) withCleanerPolicy(policy = KEEP_LATEST_COMMITS) 属性：hoodie.cleaner.policy 要使用的清理政策。Hudi将删除旧版本的parquet文件以回收空间。 任何引用此版本文件的查询和计算都将失败。最好确保数据保留的时间超过最大查询执行时间。 retainCommits(no_of_commits_to_retain = 24) 属性：hoodie.cleaner.commits.retained 保留的提交数。因此，数据将保留为num_of_commits * time_between_commits（计划的）。 这也直接转化为您可以逐步提取此数据集的数量 archiveCommitsWith(minCommits = 96, maxCommits = 128) 属性：hoodie.keep.min.commits, hoodie.keep.max.commits 每个提交都是.hoodie目录中的一个小文件。由于DFS通常不支持大量小文件，因此Hudi将较早的提交归档到顺序日志中。 提交通过重命名提交文件以原子方式发布。 withCommitsArchivalBatchSize(batch = 10) 属性：hoodie.commits.archival.batch 这控制着批量读取并一起归档的提交即时的数量。 compactionSmallFileSize(size = 0) 属性：hoodie.parquet.small.file.limit 该值应小于maxFileSize，如果将其设置为0，会关闭此功能。 由于批处理中分区中插入记录的数量众多，总会出现小文件。 Hudi提供了一个选项，可以通过将对该分区中的插入作为对现有小文件的更新来解决小文件的问题。 此处的大小是被视为“小文件大小”的最小文件大小。 insertSplitSize(size = 500000) 属性：hoodie.copyonwrite.insert.split.size 插入写入并行度。为单个分区的总共插入次数。 写出100MB的文件，至少1kb大小的记录，意味着每个文件有100K记录。默认值是超额配置为500K。 为了改善插入延迟，请对其进行调整以匹配单个文件中的记录数。 将此值设置为较小的值将导致文件变小（尤其是当compactionSmallFileSize为0时） autoTuneInsertSplits(true) 属性：hoodie.copyonwrite.insert.auto.split Hudi是否应该基于最后24个提交的元数据动态计算insertSplitSize。默认关闭。 approxRecordSize(size = 1024) 属性：hoodie.copyonwrite.record.size.estimate 平均记录大小。如果指定，hudi将使用它，并且不会基于最后24个提交的元数据动态地计算。 没有默认值设置。这对于计算插入并行度以及将插入打包到小文件中至关重要。如上所述。 withInlineCompaction(inlineCompaction = false) 属性：hoodie.compact.inline 当设置为true时，紧接在插入或插入更新或批量插入的提交或增量提交操作之后由摄取本身触发压缩 withMaxNumDeltaCommitsBeforeCompaction(maxNumDeltaCommitsBeforeCompaction = 10) 属性：hoodie.compact.inline.max.delta.commits 触发内联压缩之前要保留的最大增量提交数 withCompactionLazyBlockReadEnabled(true) 属性：hoodie.compaction.lazy.block.read 当CompactedLogScanner合并所有日志文件时，此配置有助于选择是否应延迟读取日志块。 选择true以使用I/O密集型延迟块读取（低内存使用），或者为false来使用内存密集型立即块读取（高内存使用） withCompactionReverseLogReadEnabled(false) 属性：hoodie.compaction.reverse.log.read HoodieLogFormatReader会从pos=0到pos=file_length向前读取日志文件。 如果此配置设置为true，则Reader会从pos=file_length到pos=0反向读取日志文件 withCleanerParallelism(cleanerParallelism = 200) 属性：hoodie.cleaner.parallelism 如果清理变慢，请增加此值。 withCompactionStrategy(compactionStrategy = org.apache.hudi.io.compact.strategy.LogFileSizeBasedCompactionStrategy) 属性：hoodie.compaction.strategy 用来决定在每次压缩运行期间选择要压缩的文件组的压缩策略。 默认情况下，Hudi选择具有累积最多未合并数据的日志文件 withTargetIOPerCompactionInMB(targetIOPerCompactionInMB = 500000) 属性：hoodie.compaction.target.io LogFileSizeBasedCompactionStrategy的压缩运行期间要花费的MB量。当压缩以内联模式运行时，此值有助于限制摄取延迟。 withTargetPartitionsPerDayBasedCompaction(targetPartitionsPerCompaction = 10) 属性：hoodie.compaction.daybased.target 由org.apache.hudi.io.compact.strategy.DayBasedCompactionStrategy使用，表示在压缩运行期间要压缩的最新分区数。 withPayloadClass(payloadClassName = org.apache.hudi.common.model.HoodieAvroPayload) 属性：hoodie.compaction.payload.class 这需要与插入/插入更新过程中使用的类相同。 就像写入一样，压缩也使用记录有效负载类将日志中的记录彼此合并，再次与基本文件合并，并生成压缩后要写入的最终记录。 指标配置 能够将Hudi指标报告给graphite。 withMetricsConfig (HoodieMetricsConfig) Hudi会发布有关每次提交、清理、回滚等的指标。 on(metricsOn = true) 属性：hoodie.metrics.on 打开或关闭发送指标。默认情况下处于启用状态。 withReporterType(reporterType = GRAPHITE) 属性：hoodie.metrics.reporter.type 指标报告者的类型。默认使用graphite，也是唯一支持的类型。 toGraphiteHost(host = localhost) 属性：hoodie.metrics.graphite.host 要连接的graphite主机 onGraphitePort(port = 4756) 属性：hoodie.metrics.graphite.port 要连接的graphite端口 usePrefix(prefix = \"\") 属性：hoodie.metrics.graphite.metric.prefix 适用于所有指标的标准前缀。这有助于添加如数据中心、环境等信息 内存配置 控制由Hudi内部执行的压缩和合并的内存使用情况 withMemoryConfig (HoodieMemoryConfig) 内存相关配置 withMaxMemoryFractionPerPartitionMerge(maxMemoryFractionPerPartitionMerge = 0.6) 属性：hoodie.memory.merge.fraction 该比例乘以用户内存比例（1-spark.memory.fraction）以获得合并期间要使用的堆空间的最终比例 withMaxMemorySizePerCompactionInBytes(maxMemorySizePerCompactionInBytes = 1GB) 属性：hoodie.memory.compaction.fraction HoodieCompactedLogScanner读取日志块，将记录转换为HoodieRecords，然后合并这些日志块和记录。 在任何时候，日志块中的条目数可以小于或等于相应的parquet文件中的条目数。这可能导致Scanner出现OOM。 因此，可溢出的映射有助于减轻内存压力。使用此配置来设置可溢出映射的最大允许inMemory占用空间。 withWriteStatusFailureFraction(failureFraction = 0.1) 属性：hoodie.memory.writestatus.failure.fraction 此属性控制报告给驱动程序的失败记录和异常的比例 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"performance.html":{"url":"performance.html","title":"性能","keywords":"","body":"在本节中，我们将介绍一些有关Hudi插入更新、增量提取的实际性能数据，并将其与实现这些任务的其它传统工具进行比较。 插入更新 下面显示了从NoSQL数据库摄取获得的速度提升，这些速度提升数据是通过在写入时复制存储上的Hudi数据集上插入更新而获得的， 数据集包括5个从小到大的表（相对于批量加载表）。 由于Hudi可以通过增量构建数据集，它也为更频繁地调度摄取提供了可能性，从而减少了延迟，并显著节省了总体计算成本。 Hudi插入更新在t1表的一次提交中就进行了高达4TB的压力测试。 有关一些调优技巧，请参见这里。 索引 为了有效地插入更新数据，Hudi需要将要写入的批量数据中的记录分类为插入和更新（并标记它所属的文件组）。 为了加快此操作的速度，Hudi采用了可插拔索引机制，该机制存储了recordKey和它所属的文件组ID之间的映射。 默认情况下，Hudi使用内置索引，该索引使用文件范围和布隆过滤器来完成此任务，相比于Spark Join，其速度最高可提高10倍。 当您将recordKey建模为单调递增时（例如时间戳前缀），Hudi提供了最佳的索引性能，从而进行范围过滤来避免与许多文件进行比较。 即使对于基于UUID的键，也有已知技术来达到同样目的。 例如，在具有80B键、3个分区、11416个文件、10TB数据的事件表上使用100M个时间戳前缀的键（5％的更新，95％的插入）时， 相比于原始Spark Join，Hudi索引速度的提升约为7倍（440秒相比于2880秒）。 即使对于具有挑战性的工作负载，如使用300个核对3.25B UUID键、30个分区、6180个文件的“100％更新”的数据库摄取工作负载，Hudi索引也可以提供80-100％的加速。 读优化查询 读优化视图的主要设计目标是在不影响查询的情况下实现上一节中提到的延迟减少和效率提高。 下图比较了对Hudi和非Hudi数据集的Hive、Presto、Spark查询，并对此进行说明。 Hive Spark Presto 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"},"admin_guide.html":{"url":"admin_guide.html","title":"管理","keywords":"","body":"管理员/运维人员可以通过以下方式了解Hudi数据集/管道 通过Admin CLI进行管理 Graphite指标 Hudi应用程序的Spark UI 本节简要介绍了每一种方法，并提供了有关故障排除的一些常规指南 Admin CLI 一旦构建了hudi，就可以通过cd hudi-cli && ./hudi-cli.sh启动shell。 一个hudi数据集位于DFS上的basePath位置，我们需要该位置才能连接到Hudi数据集。 Hudi库使用.hoodie子文件夹跟踪所有元数据，从而有效地在内部管理该数据集。 初始化hudi表，可使用如下命令。 18/09/06 15:56:52 INFO annotation.AutowiredAnnotationBeanPostProcessor: JSR-330 'javax.inject.Inject' annotation found and supported for autowiring ============================================ * * * _ _ _ _ * * | | | | | | (_) * * | |__| | __| | - * * | __ || | / _` | || * * | | | || || (_| | || * * |_| |_|\\___/ \\____/ || * * * ============================================ Welcome to Hoodie CLI. Please type help if you are looking for help. hudi->create --path /user/hive/warehouse/table1 --tableName hoodie_table_1 --tableType COPY_ON_WRITE ..... 18/09/06 15:57:15 INFO table.HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from ... 使用desc命令可以查看hudi表的描述信息: hoodie:hoodie_table_1->desc 18/09/06 15:57:19 INFO timeline.HoodieActiveTimeline: Loaded instants [] _________________________________________________________ | Property | Value | |========================================================| | basePath | ... | | metaPath | ... | | fileSystem | hdfs | | hoodie.table.name | hoodie_table_1 | | hoodie.table.type | COPY_ON_WRITE | | hoodie.archivelog.folder| | 以下是连接到包含uber trips的Hudi数据集的示例命令。 hoodie:trips->connect --path /app/uber/trips 16/10/05 23:20:37 INFO model.HoodieTableMetadata: Attempting to load the commits under /app/uber/trips/.hoodie with suffix .commit 16/10/05 23:20:37 INFO model.HoodieTableMetadata: Attempting to load the commits under /app/uber/trips/.hoodie with suffix .inflight 16/10/05 23:20:37 INFO model.HoodieTableMetadata: All commits :HoodieCommits{commitList=[20161002045850, 20161002052915, 20161002055918, 20161002065317, 20161002075932, 20161002082904, 20161002085949, 20161002092936, 20161002105903, 20161002112938, 20161002123005, 20161002133002, 20161002155940, 20161002165924, 20161002172907, 20161002175905, 20161002190016, 20161002192954, 20161002195925, 20161002205935, 20161002215928, 20161002222938, 20161002225915, 20161002232906, 20161003003028, 20161003005958, 20161003012936, 20161003022924, 20161003025859, 20161003032854, 20161003042930, 20161003052911, 20161003055907, 20161003062946, 20161003065927, 20161003075924, 20161003082926, 20161003085925, 20161003092909, 20161003100010, 20161003102913, 20161003105850, 20161003112910, 20161003115851, 20161003122929, 20161003132931, 20161003142952, 20161003145856, 20161003152953, 20161003155912, 20161003162922, 20161003165852, 20161003172923, 20161003175923, 20161003195931, 20161003210118, 20161003212919, 20161003215928, 20161003223000, 20161003225858, 20161004003042, 20161004011345, 20161004015235, 20161004022234, 20161004063001, 20161004072402, 20161004074436, 20161004080224, 20161004082928, 20161004085857, 20161004105922, 20161004122927, 20161004142929, 20161004163026, 20161004175925, 20161004194411, 20161004203202, 20161004211210, 20161004214115, 20161004220437, 20161004223020, 20161004225321, 20161004231431, 20161004233643, 20161005010227, 20161005015927, 20161005022911, 20161005032958, 20161005035939, 20161005052904, 20161005070028, 20161005074429, 20161005081318, 20161005083455, 20161005085921, 20161005092901, 20161005095936, 20161005120158, 20161005123418, 20161005125911, 20161005133107, 20161005155908, 20161005163517, 20161005165855, 20161005180127, 20161005184226, 20161005191051, 20161005193234, 20161005203112, 20161005205920, 20161005212949, 20161005223034, 20161005225920]} Metadata for table trips loaded hoodie:trips-> 连接到数据集后，便可使用许多其他命令。该shell程序具有上下文自动完成帮助(按TAB键)，下面是所有命令的列表，本节中对其中的一些命令进行了详细示例。 hoodie:trips->help * ! - Allows execution of operating system (OS) commands * // - Inline comment markers (start of line only) * ; - Inline comment markers (start of line only) * addpartitionmeta - Add partition metadata to a dataset, if not present * clear - Clears the console * cls - Clears the console * commit rollback - Rollback a commit * commits compare - Compare commits with another Hoodie dataset * commit showfiles - Show file level details of a commit * commit showpartitions - Show partition level details of a commit * commits refresh - Refresh the commits * commits show - Show the commits * commits sync - Compare commits with another Hoodie dataset * connect - Connect to a hoodie dataset * date - Displays the local date and time * exit - Exits the shell * help - List all commands usage * quit - Exits the shell * records deduplicate - De-duplicate a partition path contains duplicates & produce repaired files to replace with * script - Parses the specified resource file and executes its commands * stats filesizes - File Sizes. Display summary stats on sizes of files * stats wa - Write Amplification. Ratio of how many records were upserted to how many records were actually written * sync validate - Validate the sync by counting the number of records * system properties - Shows the shell's properties * utils loadClass - Load a class * version - Displays shell version hoodie:trips-> 检查提交 在Hudi中，更新或插入一批记录的任务被称为提交。提交可提供基本的原子性保证，即只有提交的数据可用于查询。 每个提交都有一个单调递增的字符串/数字，称为提交编号。通常，这是我们开始提交的时间。 查看有关最近10次提交的一些基本信息， hoodie:trips->commits show --sortBy \"Total Bytes Written\" --desc true --limit 10 ________________________________________________________________________________________________________________________________________________________________________ | CommitTime | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors| |=======================================================================================================================================================================| .... .... .... hoodie:trips-> 在每次写入开始时，Hudi还将.inflight提交写入.hoodie文件夹。您可以使用那里的时间戳来估计正在进行的提交已经花费的时间 $ hdfs dfs -ls /app/uber/trips/.hoodie/*.inflight -rw-r--r-- 3 vinoth supergroup 321984 2016-10-05 23:18 /app/uber/trips/.hoodie/20161005225920.inflight 深入到特定的提交 了解写入如何分散到特定分区， hoodie:trips->commit showpartitions --commit 20161005165855 --sortBy \"Total Bytes Written\" --desc true --limit 10 __________________________________________________________________________________________________________________________________________ | Partition Path| Total Files Added| Total Files Updated| Total Records Inserted| Total Records Updated| Total Bytes Written| Total Errors| |=========================================================================================================================================| .... .... 如果您需要文件级粒度，我们可以执行以下操作 hoodie:trips->commit showfiles --commit 20161005165855 --sortBy \"Partition Path\" ________________________________________________________________________________________________________________________________________________________ | Partition Path| File ID | Previous Commit| Total Records Updated| Total Records Written| Total Bytes Written| Total Errors| |=======================================================================================================================================================| .... .... 文件系统视图 Hudi将每个分区视为文件组的集合，每个文件组包含按提交顺序排列的文件切片列表(请参阅概念)。以下命令允许用户查看数据集的文件切片。 hoodie:stock_ticks_mor->show fsview all .... _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ | Partition | FileId | Base-Instant | Data-File | Data-File Size| Num Delta Files| Total Delta File Size| Delta Files | |==============================================================================================================================================================================================================================================================================================================================================================================================================| | 2018/08/31| 111415c3-f26d-4639-86c8-f9956f245ac3| 20181002180759| hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/111415c3-f26d-4639-86c8-f9956f245ac3_0_20181002180759.parquet| 432.5 KB | 1 | 20.8 KB | [HoodieLogFile {hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/.111415c3-f26d-4639-86c8-f9956f245ac3_20181002180759.log.1}]| hoodie:stock_ticks_mor->show fsview latest --partitionPath \"2018/08/31\" ...... __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ | Partition | FileId | Base-Instant | Data-File | Data-File Size| Num Delta Files| Total Delta Size| Delta Size - compaction scheduled| Delta Size - compaction unscheduled| Delta To Base Ratio - compaction scheduled| Delta To Base Ratio - compaction unscheduled| Delta Files - compaction scheduled | Delta Files - compaction unscheduled| |=================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================| | 2018/08/31| 111415c3-f26d-4639-86c8-f9956f245ac3| 20181002180759| hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/111415c3-f26d-4639-86c8-f9956f245ac3_0_20181002180759.parquet| 432.5 KB | 1 | 20.8 KB | 20.8 KB | 0.0 B | 0.0 B | 0.0 B | [HoodieLogFile {hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/.111415c3-f26d-4639-86c8-f9956f245ac3_20181002180759.log.1}]| [] | hoodie:stock_ticks_mor-> 统计信息 由于Hudi直接管理DFS数据集的文件大小，这些信息会帮助你全面了解Hudi的运行状况 hoodie:trips->stats filesizes --partitionPath 2016/09/01 --sortBy \"95th\" --desc true --limit 10 ________________________________________________________________________________________________ | CommitTime | Min | 10th | 50th | avg | 95th | Max | NumFiles| StdDev | |===============================================================================================| | | 93.9 MB | 93.9 MB | 93.9 MB | 93.9 MB | 93.9 MB | 93.9 MB | 2 | 2.3 KB | .... .... 如果Hudi写入花费的时间更长，那么可以通过观察写放大指标来发现任何异常 hoodie:trips->stats wa __________________________________________________________________________ | CommitTime | Total Upserted| Total Written| Write Amplifiation Factor| |=========================================================================| .... .... 归档的提交 为了限制DFS上.commit文件的增长量，Hudi将较旧的.commit文件(适当考虑清理策略)归档到commits.archived文件中。 这是一个序列文件，其包含commitNumber => json的映射，及有关提交的原始信息(上面已很好地汇总了相同的信息)。 压缩 要了解压缩和写程序之间的时滞，请使用以下命令列出所有待处理的压缩。 hoodie:trips->compactions show all ___________________________________________________________________ | Compaction Instant Time| State | Total FileIds to be Compacted| |==================================================================| | | REQUESTED| 35 | | | INFLIGHT | 27 | 要检查特定的压缩计划，请使用 hoodie:trips->compaction show --instant _________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ | Partition Path| File Id | Base Instant | Data File Path | Total Delta Files| getMetrics | |================================================================================================================================================================================================================================================ | 2018/07/17 | | | viewfs://ns-default/.../../UUID_.parquet | 1 | {TOTAL_LOG_FILES=1.0, TOTAL_IO_READ_MB=1230.0, TOTAL_LOG_FILES_SIZE=2.51255751E8, TOTAL_IO_WRITE_MB=991.0, TOTAL_IO_MB=2221.0}| 要手动调度或运行压缩，请使用以下命令。该命令使用spark启动器执行压缩操作。 注意：确保没有其他应用程序正在同时调度此数据集的压缩 hoodie:trips->help compaction schedule Keyword: compaction schedule Description: Schedule Compaction Keyword: sparkMemory Help: Spark executor memory Mandatory: false Default if specified: '__NULL__' Default if unspecified: '1G' * compaction schedule - Schedule Compaction hoodie:trips->help compaction run Keyword: compaction run Description: Run Compaction for given instant time Keyword: tableName Help: Table name Mandatory: true Default if specified: '__NULL__' Default if unspecified: '__NULL__' Keyword: parallelism Help: Parallelism for hoodie compaction Mandatory: true Default if specified: '__NULL__' Default if unspecified: '__NULL__' Keyword: schemaFilePath Help: Path for Avro schema file Mandatory: true Default if specified: '__NULL__' Default if unspecified: '__NULL__' Keyword: sparkMemory Help: Spark executor memory Mandatory: true Default if specified: '__NULL__' Default if unspecified: '__NULL__' Keyword: retry Help: Number of retries Mandatory: true Default if specified: '__NULL__' Default if unspecified: '__NULL__' Keyword: compactionInstant Help: Base path for the target hoodie dataset Mandatory: true Default if specified: '__NULL__' Default if unspecified: '__NULL__' * compaction run - Run Compaction for given instant time 验证压缩 验证压缩计划：检查压缩所需的所有文件是否都存在且有效 hoodie:stock_ticks_mor->compaction validate --instant 20181005222611 ... COMPACTION PLAN VALID ___________________________________________________________________________________________________________________________________________________________________________________________________________________________ | File Id | Base Instant Time| Base Data File | Num Delta Files| Valid| Error| |==========================================================================================================================================================================================================================| | 05320e98-9a57-4c38-b809-a6beaaeb36bd| 20181005222445 | hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/05320e98-9a57-4c38-b809-a6beaaeb36bd_0_20181005222445.parquet| 1 | true | | hoodie:stock_ticks_mor->compaction validate --instant 20181005222601 COMPACTION PLAN INVALID _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ | File Id | Base Instant Time| Base Data File | Num Delta Files| Valid| Error | |=====================================================================================================================================================================================================================================================================================================| | 05320e98-9a57-4c38-b809-a6beaaeb36bd| 20181005222445 | hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/05320e98-9a57-4c38-b809-a6beaaeb36bd_0_20181005222445.parquet| 1 | false| All log files specified in compaction operation is not present. Missing .... | 注意 必须在其他写入/摄取程序没有运行的情况下执行以下命令。 有时，有必要从压缩计划中删除fileId以便加快或取消压缩操作。 压缩计划之后在此文件上发生的所有新日志文件都将被安全地重命名以便进行保留。Hudi提供以下CLI来支持 取消调度压缩 hoodie:trips->compaction unscheduleFileId --fileId .... No File renames needed to unschedule file from pending compaction. Operation successful. 在其他情况下，需要撤销整个压缩计划。以下CLI支持此功能 hoodie:trips->compaction unschedule --compactionInstant ..... No File renames needed to unschedule pending compaction. Operation successful. 修复压缩 上面的压缩取消调度操作有时可能会部分失败(例如：DFS暂时不可用)。 如果发生部分故障，则压缩操作可能与文件切片的状态不一致。 当您运行压缩验证时，您会注意到无效的压缩操作(如果有的话)。 在这种情况下，修复命令将立即执行，它将重新排列文件切片，以使文件不丢失，并且文件切片与压缩计划一致 hoodie:stock_ticks_mor->compaction repair --instant 20181005222611 ...... Compaction successfully repaired ..... 指标 为Hudi Client配置正确的数据集名称和指标环境后，它将生成以下graphite指标，以帮助调试hudi数据集 提交持续时间 - 这是成功提交一批记录所花费的时间 回滚持续时间 - 同样，撤消失败的提交所剩余的部分数据所花费的时间(每次写入失败后都会自动发生) 文件级别指标 - 显示每次提交中新增、版本、删除(清除)的文件数量 记录级别指标 - 每次提交插入/更新的记录总数 分区级别指标 - 更新的分区数量(对于了解提交持续时间的突然峰值非常有用) 然后可以将这些指标绘制在grafana等标准工具上。以下是提交持续时间图表示例。 故障排除 以下部分通常有助于调试Hudi故障。以下元数据已被添加到每条记录中，可以通过标准Hadoop SQL引擎(Hive/Presto/Spark)检索，来更容易地诊断问题的严重性。 _hoodie_record_key - 作为每个DFS分区内的主键，是所有更新/插入的基础 _hoodie_commit_time - 该记录上次的提交 _hoodie_file_name - 包含记录的实际文件名(对检查重复非常有用) _hoodie_partition_path - basePath的路径，该路径标识包含此记录的分区 请注意，到目前为止，Hudi假定应用程序为给定的recordKey传递相同的确定性分区路径。即仅在每个分区内保证recordKey(主键)的唯一性。 缺失记录 请在可能写入记录的窗口中，使用上面的admin命令检查是否存在任何写入错误。 如果确实发现错误，那么记录实际上不是由Hudi写入的，而是交还给应用程序来决定如何处理。 重复 首先，请确保访问Hudi数据集的查询是没有问题的，并之后确认的确有重复。 如果确认，请使用上面的元数据字段来标识包含记录的物理文件和分区文件。 如果重复的记录存在于不同分区路径下的文件，则意味着您的应用程序正在为同一recordKey生成不同的分区路径，请修复您的应用程序. 如果重复的记录存在于同一分区路径下的多个文件，请使用邮件列表汇报这个问题。这不应该发生。您可以使用records deduplicate命令修复数据。 Spark故障 典型的upsert() DAG如下所示。请注意，Hudi客户端会缓存中间的RDD，以智能地并调整文件大小和Spark并行度。 另外，由于还显示了探针作业，Spark UI显示了两次sortByKey，但它只是一个排序。 概括地说，有两个步骤 索引查找以标识要更改的文件 Job 1 : 触发输入数据读取，转换为HoodieRecord对象，然后根据输入记录拿到目标分区路径。 Job 2 : 加载我们需要检查的文件名集。 Job 3 & 4 : 通过联合上面1和2中的RDD，智能调整spark join并行度，然后进行实际查找。 Job 5 : 生成带有位置的recordKeys作为标记的RDD。 执行数据的实际写入 Job 6 : 将记录与recordKey(位置)进行懒惰连接，以提供最终的HoodieRecord集，现在它包含每条记录的文件/分区路径信息(如果插入，则为null)。然后还要再次分析工作负载以确定文件的大小。 Job 7 : 实际写入数据(更新 + 插入 + 插入转为更新以保持文件大小) 根据异常源(Hudi/Spark)，上述关于DAG的信息可用于查明实际问题。最常遇到的故障是由YARN/DFS临时故障引起的。 将来，将在项目中添加更复杂的调试/管理UI，以帮助自动进行某些调试。 我们一直在努力 apachecn/hudi-doc-zh (adsbygoogle = window.adsbygoogle || []).push({}); var _hmt = _hmt || []; (function() { var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?2574ec73e5e1748bda66f689cac02272\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s); })(); window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-102475051-13'); const gitalk = new Gitalk({ clientID: '0c7afcd52f7c96b1c230', clientSecret: '88920ce6e8b31a24a61c3762b9947337b21a7806', repo: 'hudi-doc-zh', owner: 'apachecn', admin: ['jiangzhonglian', 'wizardforcel'], id: md5(location.pathname), distractionFreeMode: false }) gitalk.render('gitalk-container') Copyright © ibooker.org.cn 2019 all right reserved，由 ApacheCN 团队提供支持该文件修订时间： 2020-01-09 14:15:24 console.log(\"plugin-popup....\");document.onclick = function(e){ e.target.tagName === \"IMG\" && window.open(e.target.src,e.target.src)}img{cursor:pointer}"}}